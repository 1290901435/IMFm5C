{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "977710d2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-03T06:42:57.062162Z",
     "start_time": "2023-06-03T06:42:57.058487Z"
    }
   },
   "outputs": [],
   "source": [
    "# result_w2v ['BiLSTM', 'XGBoost', 'CatBoost', 'LightGBM', 'AdaBoost', 'ExtraTree']\n",
    "# result_sec ['LSTM_Attention_CNN', 'BiLSTM_Attention_CNN', 'XGBoost', 'CatBoost', 'LightGBM', 'ExtraTree']\n",
    "# result_loc ['CNN', 'LSTM', 'BiLSTM', 'LSTM_Attention_CNN', 'BiLSTM_Attention_CNN', 'SVM']\n",
    "# result_com ['CNN', 'LSTM', 'BiLSTM', 'LSTM_Attention_CNN', 'BiLSTM_Attention_CNN', 'SVM']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da60e6f6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-03T06:44:20.633696Z",
     "start_time": "2023-06-03T06:44:15.927505Z"
    }
   },
   "outputs": [],
   "source": [
    "from features import ensembleFeature\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from utils import metricsCal\n",
    "from thundersvm import SVC\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "59aeaf5c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-03T06:44:21.047092Z",
     "start_time": "2023-06-03T06:44:21.041276Z"
    }
   },
   "outputs": [],
   "source": [
    "##将序列转化为词向量用到的函数\n",
    "def fa_seq(filepath):\n",
    "    f = open(filepath,'r')\n",
    "    x = []\n",
    "    for i in f:\n",
    "        x.append(i)\n",
    "    seq = []\n",
    "    for i in range(len(x)):\n",
    "        if i%2==1:\n",
    "            seq.append(x[i][:-1])\n",
    "    return seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a8019362",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-03T06:44:26.452483Z",
     "start_time": "2023-06-03T06:44:26.338557Z"
    }
   },
   "outputs": [],
   "source": [
    "train_seq = fa_seq(\"data/Mouse/mouse_train.fasta\")\n",
    "trainLabel = np.append(np.ones(int(len(train_seq)/2)),np.zeros(int(len(train_seq)-len(train_seq)/2)),axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2064ce97",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-03T06:48:53.134373Z",
     "start_time": "2023-06-03T06:44:37.904737Z"
    }
   },
   "outputs": [],
   "source": [
    "trainData_EIIP = ensembleFeature.EIIP(train_seq)\n",
    "\n",
    "trainData_PseEIIP = ensembleFeature.PseEIIP(train_seq)\n",
    "\n",
    "trainData_PCP = ensembleFeature.PCP(train_seq)\n",
    "\n",
    "trainData_NCPA = ensembleFeature.NCPA(train_seq)\n",
    "trainData_NCPA = trainData_NCPA.reshape(trainData_NCPA.shape[0],-1)\n",
    "\n",
    "trainData_DBPF = ensembleFeature.DBPF(train_seq)\n",
    "trainData_DBPF = trainData_DBPF.reshape(trainData_DBPF.shape[0],-1)\n",
    "\n",
    "trainData_com = np.concatenate(  (trainData_EIIP,trainData_PseEIIP,trainData_PCP,trainData_NCPA,trainData_DBPF),axis=1)\n",
    "trainData = trainData_com.reshape(trainData_com.shape[0],1,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f2776af3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-03T07:05:31.482344Z",
     "start_time": "2023-06-03T07:04:33.705221Z"
    }
   },
   "outputs": [],
   "source": [
    "test_seq = fa_seq(\"data/Mouse/mouse_indep.fasta\")\n",
    "testLabel = np.append(np.ones(int(len(test_seq)/2)),np.zeros(int(len(test_seq)-len(test_seq)/2)),axis=0)\n",
    "\n",
    "testData_EIIP = ensembleFeature.EIIP(test_seq)\n",
    "\n",
    "testData_PseEIIP = ensembleFeature.PseEIIP(test_seq)\n",
    "\n",
    "testData_PCP = ensembleFeature.PCP(test_seq)\n",
    "\n",
    "testData_NCPA = ensembleFeature.NCPA(test_seq)\n",
    "testData_NCPA = testData_NCPA.reshape(testData_NCPA.shape[0],-1)\n",
    "\n",
    "testData_DBPF = ensembleFeature.DBPF(test_seq)\n",
    "testData_DBPF = testData_DBPF.reshape(testData_DBPF.shape[0],-1)\n",
    "\n",
    "testData_com = np.concatenate(  (testData_EIIP,testData_PseEIIP,testData_PCP,testData_NCPA,testData_DBPF),axis=1)\n",
    "testData = testData_com.reshape(testData_com.shape[0],1,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b19b2c38",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-03T06:48:53.167144Z",
     "start_time": "2023-06-03T06:48:53.149082Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def attention(query, key, value, mask=None, dropout=None):  # q,k,v: [batch, h, seq_len, d_k]\n",
    "    d_k = query.size(-1)  # query的维度\n",
    "    scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)  # 打分机制 [batch, h, seq_len, seq_len]\n",
    "    p_atten = F.softmax(scores, dim=-1)  # 对最后一个维度归一化得分, [batch, h, seq_len, seq_len]\n",
    "    if dropout is not None:\n",
    "        p_atten = dropout(p_atten)\n",
    "    return torch.matmul(p_atten, value), p_atten  # [batch, h, seq_len, d_k] 作矩阵的乘法\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, dim1, dim2, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "\n",
    "        #if dim % 2 != 0:\n",
    "        #    raise ValueError(\"Cannot use sin/cos positional encoding with \"\n",
    "        #                     \"odd dim (got dim={:d})\".format(dim))\n",
    "\n",
    "        \"\"\"\n",
    "        构建位置编码pe\n",
    "        pe公式为：\n",
    "        PE(pos,2i/2i+1) = sin/cos(pos/10000^{2i/d_{model}})\n",
    "        \"\"\"\n",
    "        pe = torch.zeros(max_len, dim2)  # max_len 是解码器生成句子的最长的长度，假设是 10\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        div_term0 = torch.exp((torch.arange(0, dim2, 2, dtype=torch.float) * -(math.log(10000.0) / dim2)))\n",
    "        div_term1 = torch.exp((torch.arange(1, dim2, 2, dtype=torch.float) * -(math.log(10000.0) / dim2)))\n",
    "        \n",
    "        pe[:, 0::2] = torch.sin(position.float() * div_term0)\n",
    "        pe[:, 1::2] = torch.cos(position.float() * div_term1)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "        #self.drop_out = nn.Dropout(p=dropout)\n",
    "        self.dim2 = dim2\n",
    "        self.bm1 = nn.BatchNorm1d(dim1,eps=1e-05)\n",
    "\n",
    "    def forward(self, emb, step=None):\n",
    "        emb = emb * math.sqrt(self.dim2)\n",
    "        if step is None:\n",
    "            emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
    "        else:\n",
    "            emb = emb + self.pe[step]\n",
    "        #emb = self.drop_out(emb)\n",
    "        emb = self.bm1(emb.to(torch.float32))\n",
    "        return emb\n",
    "    \n",
    "def clones(module, N):  #定义clones方法\n",
    "    return nn.ModuleList([copy.deepcopy(module)\n",
    "                          for _ in range(N)])  #让原来变量不影响,且克隆module N次\n",
    "\n",
    "class SelfAttention(nn.Module):  #多头注意力机制\n",
    "\n",
    "    def __init__(self,embedding_dim, dropout=0.1):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.linears = clones(nn.Linear(embedding_dim, embedding_dim), 4)  #克隆四份Linear网络层\n",
    "        self.dropout = nn.Dropout(p=dropout)  #定义Dropout层\n",
    "\n",
    "    def forward(self,query,key,value,mask=None):  # q,k,v: [batch, seq_len, embedding_dim]\n",
    "        nbatches = query.shape[0]  #批数量\n",
    "        query, key, value = [\n",
    "            l(x) for l, x in zip(self.linears,\n",
    "                            (query.to(torch.float32),\n",
    "                             key.to(torch.float32),\n",
    "                             value.to(torch.float32)))\n",
    "        ]  #获取zip的query,key,value权重矩阵\n",
    "        attn, p_atten = attention(query,key,value,mask=mask,dropout=self.dropout)\n",
    "        out = self.linears[-1](attn)  #得到最后一层线性层的输出\n",
    "        return out,p_atten  #返回out结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c66fe0ef",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-03T06:48:53.271147Z",
     "start_time": "2023-06-03T06:48:53.180436Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "from utils import metricsCal\n",
    "from torch.utils.data import DataLoader,TensorDataset\n",
    "import copy\n",
    "from torch.autograd import Variable\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "def train(model,data,label,epoch,train_device,model_dir,batch_size):\n",
    "    if os.path.exists(model_dir+'model.pt'):\n",
    "        model_train = torch.load(model_dir+'/model.pt')\n",
    "    else:\n",
    "        model_train = model\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = torch.optim.Adam(model_train.parameters(),lr=0.003)  #改变学习率\n",
    "    dataX = torch.Tensor(data).clone().detach()\n",
    "    label = torch.Tensor(label).clone().detach()\n",
    "    train_data = TensorDataset(dataX, label)\n",
    "    train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
    "    running_loss = 0.0\n",
    "    model_train = model_train.to(train_device)\n",
    "    for batch_idx,data in enumerate(train_loader,0):\n",
    "        inputs,target = data\n",
    "        #inputs = inputs.reshape(inputs.shape[0], 1, inputs.shape[1])\n",
    "        inputs = inputs.to(train_device)\n",
    "        target = target.to(train_device)\n",
    "        target = target.reshape(target.shape[0],1)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model_train(inputs)\n",
    "        loss = criterion(outputs,target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        #print(type(loss))\n",
    "        running_loss += loss.item()\n",
    "        if batch_idx == len(dataX)//batch_size:\n",
    "            #print('[%d, %5d] epoch loss: %.3f' %(epoch+1,batch_idx+1,running_loss))\n",
    "            print(running_loss)\n",
    "    save_model(model_train,model_dir)\n",
    "    model_train = torch.load(model_dir+'/model.pt')\n",
    "    #device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")  # 选择设备\n",
    "    th,_,_,_,_,_,_,_,_,_ = metricsCal.evaluate(model_train,train_loader,train_device)\n",
    "    return running_loss,th\n",
    "\n",
    "def test(data,label,best_auc,test_device,model_dir,batch_size,th):\n",
    "    model_test = load_model(model_dir)\n",
    "    #model_test.eval()\n",
    "    #model_test.to(test_device)\n",
    "    data = torch.Tensor(data).clone().detach()#torch.Tensor(data)\n",
    "    label = torch.Tensor(label).clone().detach()#.requires_grad_(True)torch.Tensor(label)\n",
    "    test_data = TensorDataset(data,label)\n",
    "    test_loader = DataLoader(test_data, shuffle=False, batch_size=batch_size)\n",
    "    #device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")  # 选择设备\n",
    "    _,_,_,_,Sen,Spe, Acc, mcc, AUC = metricsCal.evaluate(model_test,test_loader,test_device,False,th)\n",
    "    \n",
    "    print('Accuracy on test set: %d %%' %Acc)\n",
    "    print('Sensitivity on test set: %d %%' %Sen)\n",
    "    print('Speciality on test set: %d %%' %Spe)\n",
    "    print('MCC on test set: %.3f' %mcc)\n",
    "    print('auc on test set: %.3f' %AUC)\n",
    "    if(AUC > best_auc):\n",
    "        torch.save(model_test,model_dir+'model_best.pt')\n",
    "    return Sen,Spe,Acc, mcc, AUC\n",
    "\n",
    "def independTest(data,label,test_device,model_dir,batch_size,th):\n",
    "    model_test = load_bestModel(model_dir)\n",
    "    data = torch.Tensor(data).clone().detach()#torch.Tensor(data)\n",
    "    label = torch.Tensor(label).clone().detach()#.requires_grad_(True)torch.Tensor(label)\n",
    "    test_data = TensorDataset(data,label)\n",
    "    test_loader = DataLoader(test_data, shuffle=False, batch_size=batch_size)\n",
    "    #device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")  # 选择设备\n",
    "    _,_,_,_,Sen,Spe, Acc, mcc, AUC = metricsCal.evaluate(model_test,test_loader,test_device,False,th)\n",
    "    print(Acc,mcc,AUC)\n",
    "    print('Accuracy on test set: %d %%' %Acc)\n",
    "    print('Sensitivity on test set: %d %%' %Sen)\n",
    "    print('Speciality on test set: %d %%' %Spe)\n",
    "    print('MCC on test set: %.3f' %mcc)\n",
    "    print('auc on test set: %.3f' %AUC)\n",
    "    return Acc, mcc, AUC\n",
    "\n",
    "def load_model(model_dir):\n",
    "    if os.path.exists(model_dir+'model.pt'):\n",
    "        model_load = torch.load(model_dir+'model.pt')\n",
    "    return model_load\n",
    "\n",
    "def save_model(model_save,model_dir):\n",
    "    if not os.path.exists(model_dir):\n",
    "        os.makedirs(model_dir)\n",
    "    torch.save(model_save, model_dir+'model.pt')\n",
    "\n",
    "def load_bestModel(model_dir):\n",
    "    if os.path.exists(model_dir+'model_best.pt'):\n",
    "        model_load = torch.load(model_dir+'model_best.pt')#,map_location='cuda:0')\n",
    "    return model_load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "99525c21",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-03T06:48:53.366054Z",
     "start_time": "2023-06-03T06:48:53.291828Z"
    }
   },
   "outputs": [],
   "source": [
    "#CNN\n",
    "class Model_CNN(nn.Module):\n",
    "    def __init__(self, dim1, dim2, dropout=0.1):\n",
    "        super(Model_CNN, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(1,3,kernel_size=5,stride=2,padding=2)\n",
    "        self.conv2 = nn.Conv2d(3,3,kernel_size=5,stride=2,padding=2)\n",
    "        #self.conv3 = nn.Conv2d(3,3,kernel_size=5,stride=2,padding=2)\n",
    "        \n",
    "        k1 = int((dim1+2*2-5)/2)+1\n",
    "        k2 = int((dim2+2*2-5)/2)+1\n",
    "        k1 = int((k1+2*2-5)/2)+1\n",
    "        k2 = int((k2+2*2-5)/2)+1\n",
    "        #k1 = int((k1+2*2-5)/2)+1\n",
    "        #k2 = int((k2+2*2-5)/2)+1\n",
    "        \n",
    "        self.fn1 = nn.Linear(3*k1*k2,128)\n",
    "        self.fn2 = nn.Linear(128,1)\n",
    "        self.ac = nn.Sigmoid()\n",
    "        self.bm1 = nn.BatchNorm1d(dim1)\n",
    "        \n",
    "        self.relu = nn.ReLU()#inplace=True)\n",
    "        self.drop = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x1 = x.contiguous().view(-1,1,x.shape[1],x.shape[2])\n",
    "        x1 = self.conv1(x1)\n",
    "        x1 = self.relu(x1)\n",
    "        x1 = self.conv2(x1)\n",
    "        x1 = self.relu(x1)\n",
    "        #x1 = self.conv3(x1)\n",
    "        #x1 = self.relu(x1)\n",
    "        x1 = x1.contiguous().view(x1.shape[0],-1)\n",
    "        x1 = self.fn1(x1)\n",
    "        x1 = self.relu(x1)\n",
    "        x1 = self.drop(x1)\n",
    "        x1 = self.fn2(x1)\n",
    "        out = self.ac(x1)\n",
    "        return out\n",
    "\n",
    "#LSTM\n",
    "class Model_LSTM(nn.Module):\n",
    "    def __init__(self, dim1, dim2, dropout=0.1):\n",
    "        super(Model_LSTM, self).__init__()\n",
    "        self.posi = PositionalEncoding(dim1,dim2,dropout)\n",
    "        self.lstm = nn.LSTM(input_size=dim2,hidden_size=dim2,batch_first=True,bidirectional=False)\n",
    "        self.bm1 = nn.BatchNorm1d(dim1)\n",
    "        self.conv1 = nn.Conv2d(1,1,kernel_size=5,stride=2,padding=2)\n",
    "        k1 = int((dim1+2*2-5)/2)+1\n",
    "        k2 = int((dim2+2*2-5)/2)+1\n",
    "        self.fn1 = nn.Linear(1*k1*k2,128)\n",
    "        #self.fn1 = nn.Linear(dim1*dim2,128)\n",
    "        self.fn2 = nn.Linear(128,1)\n",
    "        self.ac = nn.Sigmoid()\n",
    "        \n",
    "        self.relu = nn.ReLU()#inplace=True)\n",
    "        self.drop = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #所有输入，如果只有一维，需要变成1*n维\n",
    "        x1 = self.posi(x)\n",
    "        x1,(h_n,c_n) = self.lstm(x1)\n",
    "        x1 = self.relu(x1)\n",
    "        x1 = self.bm1(x1)\n",
    "        \n",
    "        x1 = x1.contiguous().view(-1,1,x1.shape[1],x1.shape[2])\n",
    "        x1 = self.conv1(x1)\n",
    "        x1 = self.relu(x1)\n",
    "        x1 = x1.contiguous().view(x1.shape[0],-1)\n",
    "        x1 = self.fn1(x1)\n",
    "        x1 = self.relu(x1)\n",
    "        x1 = self.drop(x1)\n",
    "        x1 = self.fn2(x1)\n",
    "        out = self.ac(x1)\n",
    "        return out\n",
    "\n",
    "#BiLSTM\n",
    "class Model_BiLSTM(nn.Module):\n",
    "    def __init__(self, dim1, dim2, dropout=0.1):\n",
    "        super(Model_BiLSTM, self).__init__()\n",
    "        self.posi = PositionalEncoding(dim1,dim2,dropout)\n",
    "        self.lstm = nn.LSTM(input_size=dim2,hidden_size=dim2,batch_first=True,bidirectional=True)\n",
    "        self.bm1 = nn.BatchNorm1d(dim1)\n",
    "        self.conv1 = nn.Conv2d(1,1,kernel_size=5,stride=2,padding=2)\n",
    "        k1 = int((dim1+2*2-5)/2)+1\n",
    "        k2 = int((dim2+2*2-5)/2)+1\n",
    "        self.fn1 = nn.Linear(1*k1*k2,128)\n",
    "        self.fn2 = nn.Linear(128,1)\n",
    "        self.ac = nn.Sigmoid()\n",
    "        \n",
    "        self.relu = nn.ReLU()#inplace=True)\n",
    "        self.drop = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #所有输入，如果只有一维，需要变成1*n维\n",
    "        x1 = self.posi(x)\n",
    "        x1,(h_n,c_n) = self.lstm(x1)\n",
    "        x1 = x1[:,:,0:869]+x1[:,:,869:1738]\n",
    "        x1 = self.relu(x1)\n",
    "        x1 = self.bm1(x1)\n",
    "        x1 = x1.contiguous().view(-1,1,x1.shape[1],x1.shape[2])\n",
    "        x1 = self.conv1(x1)\n",
    "        x1 = self.relu(x1)\n",
    "        x1 = x1.contiguous().view(x1.shape[0],-1)\n",
    "        x1 = self.fn1(x1)\n",
    "        x1 = self.relu(x1)\n",
    "        x1 = self.drop(x1)\n",
    "        x1 = self.fn2(x1)\n",
    "        out = self.ac(x1)\n",
    "        return out\n",
    "    \n",
    "#Attention\n",
    "class Model_Attention(nn.Module):\n",
    "    def __init__(self, dim1, dim2, dropout=0.1):\n",
    "        super(Model_Attention, self).__init__()\n",
    "        self.posi = PositionalEncoding(dim1,dim2,dropout)\n",
    "        self.self_A = SelfAttention(dim2)\n",
    "        self.self_B = SelfAttention(dim1)\n",
    "        self.bm1 = nn.BatchNorm1d(dim1)\n",
    "        self.conv1 = nn.Conv2d(1,1,kernel_size=5,stride=2,padding=2)\n",
    "        k1 = int((dim1+2*2-5)/2)+1\n",
    "        k2 = int((dim2+2*2-5)/2)+1\n",
    "        self.fn1 = nn.Linear(1*k1*k2,128)\n",
    "        self.fn2 = nn.Linear(128,1)\n",
    "        self.ac = nn.Sigmoid()\n",
    "        self.bm1 = nn.BatchNorm1d(dim1)\n",
    "        \n",
    "        self.relu = nn.ReLU()#inplace=True)\n",
    "        self.drop = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #所有输入，如果只有一维，需要变成1*n维\n",
    "        x1 = self.posi(x)\n",
    "        if x1.shape[1] == 1:\n",
    "            x1 = x1.view(x1.shape[0],x1.shape[2],x1.shape[1])\n",
    "            x1,attn = self.self_B(x1,x1,x1)\n",
    "        else:\n",
    "            x1,attn = self.self_A(x1,x1,x1)\n",
    "        if x1.shape[2]==1:\n",
    "            x1 = x1.view(x1.shape[0],x1.shape[2],x1.shape[1])\n",
    "        x1 = self.bm1(x1)\n",
    "        x1 = self.relu(x1)\n",
    "        \n",
    "        x1 = x1.contiguous().view(-1,1,x1.shape[1],x1.shape[2])\n",
    "        x1 = self.conv1(x1)\n",
    "        x1 = self.relu(x1)\n",
    "        x1 = x1.contiguous().view(x1.shape[0],-1)\n",
    "        x1 = self.fn1(x1)\n",
    "        x1 = self.relu(x1)\n",
    "        x1 = self.drop(x1)\n",
    "        x1 = self.fn2(x1)\n",
    "        out = self.ac(x1)\n",
    "        return out\n",
    "\n",
    "#LSTM_Attention_CNN\n",
    "class Model_LSTM_Attention_CNN(nn.Module):\n",
    "    def __init__(self, dim1, dim2, dropout=0.1):\n",
    "        super(Model_LSTM_Attention_CNN, self).__init__()\n",
    "        self.posi = PositionalEncoding(dim1,dim2,dropout)\n",
    "        self.self_A = SelfAttention(dim2)\n",
    "        self.self_B = SelfAttention(dim1)\n",
    "        self.lstm = nn.LSTM(input_size=dim2,hidden_size=dim2,batch_first=True,bidirectional=False)\n",
    "        self.bm1 = nn.BatchNorm1d(dim1)\n",
    "        self.conv1 = nn.Conv2d(1,1,kernel_size=5,stride=2,padding=2)\n",
    "        k1 = int((dim1+2*2-5)/2)+1\n",
    "        k2 = int((dim2+2*2-5)/2)+1\n",
    "        self.fn1 = nn.Linear(1*k1*k2,128)\n",
    "        self.fn2 = nn.Linear(128,1)\n",
    "        self.ac = nn.Sigmoid()\n",
    "        self.bm1 = nn.BatchNorm1d(dim1)\n",
    "        \n",
    "        self.relu = nn.ReLU()#inplace=True)\n",
    "        self.drop = nn.Dropout(p=dropout)\n",
    "        #self.encoder = nn.Sequential()\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        #所有输入，如果只有一维，需要变成1*n维\n",
    "        x1 = self.posi(x)\n",
    "        x1,(h_n,c_n) = self.lstm(x1)\n",
    "        x1 = self.relu(x1)\n",
    "        if x1.shape[1] == 1:\n",
    "            x1 = x1.view(x1.shape[0],x1.shape[2],x1.shape[1])\n",
    "            x1,attn = self.self_B(x1,x1,x1)\n",
    "        else:\n",
    "            x1,attn = self.self_A(x1,x1,x1)\n",
    "        if x1.shape[2]==1:\n",
    "            x1 = x1.view(x1.shape[0],x1.shape[2],x1.shape[1])\n",
    "        x1 = self.bm1(x1)\n",
    "        x1 = self.relu(x1)\n",
    "        \n",
    "        x1 = x1.contiguous().view(-1,1,x1.shape[1],x1.shape[2])\n",
    "        x1 = self.conv1(x1)\n",
    "        x1 = self.relu(x1)\n",
    "        x1 = x1.contiguous().view(x1.shape[0],-1)\n",
    "        x1 = self.fn1(x1)\n",
    "        x1 = self.relu(x1)\n",
    "        x1 = self.drop(x1)\n",
    "        x1 = self.fn2(x1)\n",
    "        out = self.ac(x1)\n",
    "        return out\n",
    "    \n",
    "#BiLSTM_Attention_CNN\n",
    "class Model_BiLSTM_Attention_CNN(nn.Module):\n",
    "    def __init__(self, dim1, dim2, dropout=0.1):\n",
    "        super(Model_BiLSTM_Attention_CNN, self).__init__()\n",
    "        self.posi = PositionalEncoding(dim1,dim2,dropout)\n",
    "        self.self_A = SelfAttention(dim2)\n",
    "        self.self_B = SelfAttention(dim1)\n",
    "        self.lstm = nn.LSTM(input_size=dim2,hidden_size=dim2,batch_first=True,bidirectional=True)\n",
    "        self.bm1 = nn.BatchNorm1d(dim1)\n",
    "        self.conv1 = nn.Conv2d(1,1,kernel_size=5,stride=2,padding=2)\n",
    "        k1 = int((dim1+2*2-5)/2)+1\n",
    "        k2 = int((dim2+2*2-5)/2)+1\n",
    "        self.fn1 = nn.Linear(1*k1*k2,128)\n",
    "        self.fn2 = nn.Linear(128,1)\n",
    "        self.ac = nn.Sigmoid()\n",
    "        self.bm1 = nn.BatchNorm1d(dim1)\n",
    "        \n",
    "        self.relu = nn.ReLU()#inplace=True)\n",
    "        self.drop = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        #所有输入，如果只有一维，需要变成1*n维\n",
    "        x1 = self.posi(x)\n",
    "        x1,(h_n,c_n) = self.lstm(x1)\n",
    "        x1 = x1[:,:,0:869]+x1[:,:,869:1738]\n",
    "        x1 = self.relu(x1)\n",
    "        if x1.shape[1] == 1:\n",
    "            x1 = x1.view(x1.shape[0],x1.shape[2],x1.shape[1])\n",
    "            x1,attn = self.self_B(x1,x1,x1)\n",
    "        else:\n",
    "            x1,attn = self.self_A(x1,x1,x1)\n",
    "        if x1.shape[2]==1:\n",
    "            x1 = x1.view(x1.shape[0],x1.shape[2],x1.shape[1])\n",
    "        x1 = self.bm1(x1)\n",
    "        x1 = self.relu(x1)\n",
    "        \n",
    "        x1 = x1.contiguous().view(-1,1,x1.shape[1],x1.shape[2])\n",
    "        x1 = self.conv1(x1)\n",
    "        x1 = self.relu(x1)\n",
    "        x1 = x1.contiguous().view(x1.shape[0],-1)\n",
    "        x1 = self.fn1(x1)\n",
    "        x1 = self.relu(x1)\n",
    "        x1 = self.drop(x1)\n",
    "        x1 = self.fn2(x1)\n",
    "        out = self.ac(x1)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "955ba31d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-03T06:54:28.945212Z",
     "start_time": "2023-06-03T06:54:28.929744Z"
    }
   },
   "outputs": [],
   "source": [
    "def independResult(data,label,test_device,model_dir,batch_size,th=0.5):\n",
    "    model_test = load_bestModel(model_dir)\n",
    "    data = torch.Tensor(data).clone().detach()#torch.Tensor(data)\n",
    "    label = torch.Tensor(label).clone().detach()#.requires_grad_(True)torch.Tensor(label)\n",
    "    test_data = TensorDataset(data,label)\n",
    "    test_loader = DataLoader(test_data, shuffle=False, batch_size=batch_size)\n",
    "    #device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")  # 选择设备\n",
    "    \n",
    "    return evaluate_result(model_test,test_loader,test_device,False)\n",
    "\n",
    "def evaluate_result(model, dataloader, device, is_train=True, threshold=0.5):\n",
    "    model.eval()\n",
    "    y_true = torch.tensor([],dtype=torch.int)\n",
    "    y_score = torch.tensor([])\n",
    "    #for data in tqdm(dataloader):\n",
    "    model = model.to(device)\n",
    "    for data in dataloader:\n",
    "        #if not isinstance(model, Res_Net):\n",
    "        if 1==1:\n",
    "            inputs,y = data\n",
    "            inputs = inputs.to(device)\n",
    "            out = model(inputs)\n",
    "        out = out.squeeze(dim=-1)\n",
    "        #out = torch.sigmoid(out)\n",
    "        y_true = torch.cat((y_true, y.int().detach().cpu()))\n",
    "        y_score = torch.cat((y_score, out.detach().cpu()))  #detach去除梯度，然后cpu()，然后cat将其连接起来\n",
    "    y_true = y_true.numpy()\n",
    "    y_score = y_score.numpy()\n",
    "    return y_score, y_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f0b740d5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-03T07:07:44.922790Z",
     "start_time": "2023-06-03T07:07:44.917612Z"
    }
   },
   "outputs": [],
   "source": [
    "X_All = []\n",
    "Label = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "cb642c3c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-03T07:46:08.613727Z",
     "start_time": "2023-06-03T07:46:08.607860Z"
    }
   },
   "outputs": [],
   "source": [
    "trainData = trainData_com.reshape(trainData_com.shape[0],1,-1)\n",
    "testData = testData_com.reshape(testData_com.shape[0],1,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "8c42cfed",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-03T07:49:42.622802Z",
     "start_time": "2023-06-03T07:49:39.486352Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/anaconda3/envs/BERT/lib/python3.6/site-packages/sklearn/utils/validation.py:72: FutureWarning: Pass shuffle=True, random_state=0 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error\n",
      "  \"will result in an error\", FutureWarning)\n",
      "/data/xiayunpeng/anaconda3/envs/BERT/lib/python3.6/site-packages/ipykernel_launcher.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/data/xiayunpeng/anaconda3/envs/BERT/lib/python3.6/site-packages/ipykernel_launcher.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/data/xiayunpeng/anaconda3/envs/BERT/lib/python3.6/site-packages/ipykernel_launcher.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/data/xiayunpeng/anaconda3/envs/BERT/lib/python3.6/site-packages/ipykernel_launcher.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/data/xiayunpeng/anaconda3/envs/BERT/lib/python3.6/site-packages/ipykernel_launcher.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/data/xiayunpeng/anaconda3/envs/BERT/lib/python3.6/site-packages/ipykernel_launcher.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/data/xiayunpeng/anaconda3/envs/BERT/lib/python3.6/site-packages/ipykernel_launcher.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/data/xiayunpeng/anaconda3/envs/BERT/lib/python3.6/site-packages/ipykernel_launcher.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/data/xiayunpeng/anaconda3/envs/BERT/lib/python3.6/site-packages/ipykernel_launcher.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/data/xiayunpeng/anaconda3/envs/BERT/lib/python3.6/site-packages/ipykernel_launcher.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "#先做交叉验证，看看多少个epoch合适\n",
    "rows = 10\n",
    "row = -1\n",
    "max_epochs = 3\n",
    "max_patience = 25\n",
    "batch_size = 256\n",
    "\n",
    "# X_BiLSTM_Attention_CNN = []\n",
    "# test_BiLSTM_Attention_CNN = []\n",
    "\n",
    "#然后全部all_in 一个模型出来\n",
    "#%env CUDA_LAUNCH_BLOCKING=1\n",
    "kf = KFold(10,True,0)\n",
    "x = 0\n",
    "#第三重循环\n",
    "for i,[train_index, test_index] in enumerate(kf.split(trainData)):\n",
    "\n",
    "    X_train = trainData[train_index]\n",
    "    X_test = trainData[test_index]\n",
    "    Y_train = trainLabel[train_index]\n",
    "    Y_test = trainLabel[test_index]\n",
    "    \n",
    "    model_dir = \"Model/Model_LSTM_Attention_CNN_com/KFold_\" + str(i)+\"/\"\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")  #选择设备\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "\n",
    "    y_pred,y_true = independResult(X_test,Y_test,device,model_dir,batch_size)\n",
    "    #X_BiLSTM_Attention_CNN.append(y_pred)\n",
    "    \n",
    "    test_pred,_ = independResult(testData,testLabel,device,model_dir,batch_size)\n",
    "    #test_BiLSTM_Attention_CNN.append(test_pred)\n",
    "    #Label.append(Y_test)\n",
    "    x += roc_auc_score(testLabel,test_pred)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "cf4d684a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-03T07:49:47.872122Z",
     "start_time": "2023-06-03T07:49:47.865611Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.586042"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "594eadfe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-03T08:39:16.134784Z",
     "start_time": "2023-06-03T08:39:16.111651Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trainData_loc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-105-08a06f145d80>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecomposition\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFastICA\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mica\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFastICA\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_components\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainData_loc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtrainData\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mica\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainData_loc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtestData\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mica\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtestData\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'trainData_loc' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import FastICA\n",
    "ica = FastICA(n_components= 100).fit(trainData_loc)\n",
    "trainData = ica.transform(trainData_loc)\n",
    "testData = ica.transform(testData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a4152197",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-03T07:21:13.146289Z",
     "start_time": "2023-06-03T07:21:07.701155Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/anaconda3/envs/BERT/lib/python3.6/site-packages/sklearn/utils/validation.py:72: FutureWarning: Pass shuffle=True, random_state=0 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error\n",
      "  \"will result in an error\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "kf = KFold(10,True,0)\n",
    "from thundersvm import SVC\n",
    "X_SVM = []\n",
    "test_SVM = []\n",
    "for i,[train_index, test_index] in enumerate(kf.split(trainData)):\n",
    "\n",
    "    X_train = trainData[train_index]\n",
    "    X_test = trainData[test_index]\n",
    "    Y_train = trainLabel[train_index]\n",
    "    Y_test = trainLabel[test_index]\n",
    "    \n",
    "    clf_svm = SVC(C=7.16281896075926, gamma=0.03935769127227975)\n",
    "    \n",
    "    clf_svm.fit(X_train,Y_train)\n",
    "\n",
    "    X_SVM.append(   np.column_stack(#(clf_rf.predict_proba(X_test)[:,1],\n",
    "               # clf_et.predict_proba(X_test)[:,1]))\n",
    "               # clf_lgbm.predict_proba(X_test)[:,1],\n",
    "               # clf_et.predict_proba(X_test)[:,1],\n",
    "               # clf_gb.predict_proba(X_test)[:,1],\n",
    "               clf_svm.decision_function(X_test)))\n",
    "    \n",
    "    test_SVM.append(   np.column_stack(#(clf_rf.predict_proba(X_test)[:,1],\n",
    "               # clf_et.predict_proba(X_test)[:,1]))\n",
    "               # clf_lgbm.predict_proba(X_test)[:,1],\n",
    "               # clf_et.predict_proba(X_test)[:,1],\n",
    "               # clf_gb.predict_proba(X_test)[:,1],\n",
    "               clf_svm.decision_function(testData)))\n",
    "    \n",
    "    #Label.append(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2765acc2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-03T07:21:28.242400Z",
     "start_time": "2023-06-03T07:21:28.235567Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "5dd799d0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-03T07:36:24.423365Z",
     "start_time": "2023-06-03T07:36:24.389849Z"
    }
   },
   "outputs": [],
   "source": [
    "Y_kk = np.hstack((Label[0],Label[1],Label[2],Label[3],Label[4],Label[5],Label[6],Label[7],Label[8],Label[9]))\n",
    "\n",
    "valData_CNN = np.hstack((X_CNN[0],X_CNN[1],X_CNN[2],X_CNN[3],X_CNN[4],X_CNN[5],X_CNN[6],X_CNN[7],X_CNN[8],X_CNN[9]))\n",
    "test_CNN = np.vstack((test_CNN[0],test_CNN[1],test_CNN[2],test_CNN[3],test_CNN[4],test_CNN[5],test_CNN[6],test_CNN[7],test_CNN[8],test_CNN[9]))\n",
    "\n",
    "valData_LSTM = np.hstack((X_LSTM[0],X_LSTM[1],X_LSTM[2],X_LSTM[3],X_LSTM[4],X_LSTM[5],X_LSTM[6],X_LSTM[7],X_LSTM[8],X_LSTM[9]))\n",
    "test_LSTM = np.vstack((test_LSTM[0],test_LSTM[1],test_LSTM[2],test_LSTM[3],test_LSTM[4],test_LSTM[5],test_LSTM[6],test_LSTM[7],test_LSTM[8],test_LSTM[9]))\n",
    "\n",
    "valData_BiLSTM = np.hstack((X_BiLSTM[0],X_BiLSTM[1],X_BiLSTM[2],X_BiLSTM[3],X_BiLSTM[4],X_BiLSTM[5],X_BiLSTM[6],X_BiLSTM[7],X_BiLSTM[8],X_BiLSTM[9]))\n",
    "test_BiLSTM = np.vstack((test_BiLSTM[0],test_BiLSTM[1],test_BiLSTM[2],test_BiLSTM[3],test_BiLSTM[4],test_BiLSTM[5],test_BiLSTM[6],test_BiLSTM[7],test_BiLSTM[8],test_BiLSTM[9]))\n",
    "\n",
    "valData_LSTM_Attention_CNN = np.hstack((X_LSTM_Attention_CNN[0],X_LSTM_Attention_CNN[1],X_LSTM_Attention_CNN[2],X_LSTM_Attention_CNN[3],\n",
    "                                        X_LSTM_Attention_CNN[4],X_LSTM_Attention_CNN[5],X_LSTM_Attention_CNN[6],X_LSTM_Attention_CNN[7],\n",
    "                                        X_LSTM_Attention_CNN[8],X_LSTM_Attention_CNN[9]))\n",
    "test_LSTM_Attention_CNN = np.vstack((test_LSTM_Attention_CNN[0],test_LSTM_Attention_CNN[1],test_LSTM_Attention_CNN[2],test_LSTM_Attention_CNN[3],\n",
    "                                     test_LSTM_Attention_CNN[4],test_LSTM_Attention_CNN[5],test_LSTM_Attention_CNN[6],test_LSTM_Attention_CNN[7],\n",
    "                                     test_LSTM_Attention_CNN[8],test_LSTM_Attention_CNN[9]))\n",
    "\n",
    "valData_BiLSTM_Attention_CNN = np.hstack((X_BiLSTM_Attention_CNN[0],X_BiLSTM_Attention_CNN[1],X_BiLSTM_Attention_CNN[2],X_BiLSTM_Attention_CNN[3],X_BiLSTM_Attention_CNN[4],X_BiLSTM_Attention_CNN[5],X_BiLSTM_Attention_CNN[6],X_BiLSTM_Attention_CNN[7],X_BiLSTM_Attention_CNN[8],X_BiLSTM_Attention_CNN[9]))\n",
    "test_BiLSTM_Attention_CNN = np.vstack((test_BiLSTM_Attention_CNN[0],test_BiLSTM_Attention_CNN[1],test_BiLSTM_Attention_CNN[2],test_BiLSTM_Attention_CNN[3],test_BiLSTM_Attention_CNN[4],test_BiLSTM_Attention_CNN[5],test_BiLSTM_Attention_CNN[6],test_BiLSTM_Attention_CNN[7],test_BiLSTM_Attention_CNN[8],test_BiLSTM_Attention_CNN[9]))\n",
    "\n",
    "valData_SVM = np.hstack((X_SVM[0],X_SVM[1],X_SVM[2],X_SVM[3],X_SVM[4],X_SVM[5],X_SVM[6],X_SVM[7],X_SVM[8],X_SVM[9]))\n",
    "test_SVM = np.vstack((test_SVM[0],test_SVM[1],test_SVM[2],test_SVM[3],test_SVM[4],test_SVM[5],test_SVM[6],test_SVM[7],test_SVM[8],test_SVM[9]))\n",
    "\n",
    "#pd.DataFrame(np.append(valData_kk.reshape(-1,1),Y_kk.reshape(-1,1),axis=1)).to_csv(\"\",header=[\"pred\",\"true\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "8f8030c4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-03T07:40:08.653988Z",
     "start_time": "2023-06-03T07:40:08.486940Z"
    }
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(np.vstack((valData_CNN,valData_LSTM,valData_BiLSTM,valData_LSTM_Attention_CNN,valData_BiLSTM_Attention_CNN,valData_SVM,Y_kk)).T).to_csv(\"Result/com_result_val.csv\",header=['CNN','LSTM','BiLSTM','LSTM_Attention_CNN','BiLSTM_Attention_CNN','SVM','True'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "29bb2e03",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-03T07:51:20.260932Z",
     "start_time": "2023-06-03T07:51:20.208599Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8219631852869096"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from sklearn.metrics import roc_auc_score\n",
    "# roc_auc_score(np.array(pd.read_csv(\"Result/com_result_val.csv\").iloc[:,-1]),np.mean(pd.read_csv(\"Result/com_result_val.csv\").iloc[:,1:6],axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "76a53f56",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-03T08:22:09.011401Z",
     "start_time": "2023-06-03T08:22:08.719898Z"
    }
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(np.vstack((test_CNN,test_LSTM,test_BiLSTM,test_LSTM_Attention_CNN,test_BiLSTM_Attention_CNN,test_SVM,testLabel)).T).to_csv(\"Result/com_result_test.csv\",header=ml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "d13a4831",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-03T08:21:37.645124Z",
     "start_time": "2023-06-03T08:21:37.639051Z"
    }
   },
   "outputs": [],
   "source": [
    "kl = ['CNN', 'LSTM', 'BiLSTM', 'LSTM_Attention_CNN', 'BiLSTM_Attention_CNN', 'SVM']\n",
    "ml = []\n",
    "for i in range(len(kl)):\n",
    "    for j in range(10):\n",
    "        ml.append(str(kl[i])+\"_KFold_\"+str(j) )\n",
    "ml.append(\"True\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "656a9ff0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-03T08:22:28.024906Z",
     "start_time": "2023-06-03T08:22:27.958763Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>CNN_KFold_0</th>\n",
       "      <th>CNN_KFold_1</th>\n",
       "      <th>CNN_KFold_2</th>\n",
       "      <th>CNN_KFold_3</th>\n",
       "      <th>CNN_KFold_4</th>\n",
       "      <th>CNN_KFold_5</th>\n",
       "      <th>CNN_KFold_6</th>\n",
       "      <th>CNN_KFold_7</th>\n",
       "      <th>CNN_KFold_8</th>\n",
       "      <th>...</th>\n",
       "      <th>SVM_KFold_1</th>\n",
       "      <th>SVM_KFold_2</th>\n",
       "      <th>SVM_KFold_3</th>\n",
       "      <th>SVM_KFold_4</th>\n",
       "      <th>SVM_KFold_5</th>\n",
       "      <th>SVM_KFold_6</th>\n",
       "      <th>SVM_KFold_7</th>\n",
       "      <th>SVM_KFold_8</th>\n",
       "      <th>SVM_KFold_9</th>\n",
       "      <th>True</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.441461</td>\n",
       "      <td>0.506231</td>\n",
       "      <td>0.296314</td>\n",
       "      <td>0.605879</td>\n",
       "      <td>0.399027</td>\n",
       "      <td>0.408772</td>\n",
       "      <td>0.738035</td>\n",
       "      <td>0.513551</td>\n",
       "      <td>0.386520</td>\n",
       "      <td>...</td>\n",
       "      <td>0.174909</td>\n",
       "      <td>0.164790</td>\n",
       "      <td>0.097380</td>\n",
       "      <td>0.081283</td>\n",
       "      <td>0.141068</td>\n",
       "      <td>0.109408</td>\n",
       "      <td>0.182129</td>\n",
       "      <td>0.158753</td>\n",
       "      <td>0.095854</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.787726</td>\n",
       "      <td>0.694683</td>\n",
       "      <td>0.722888</td>\n",
       "      <td>0.751069</td>\n",
       "      <td>0.750623</td>\n",
       "      <td>0.796548</td>\n",
       "      <td>0.744546</td>\n",
       "      <td>0.698297</td>\n",
       "      <td>0.729899</td>\n",
       "      <td>...</td>\n",
       "      <td>0.558885</td>\n",
       "      <td>0.477677</td>\n",
       "      <td>0.474592</td>\n",
       "      <td>0.500053</td>\n",
       "      <td>0.522074</td>\n",
       "      <td>0.498110</td>\n",
       "      <td>0.527881</td>\n",
       "      <td>0.445533</td>\n",
       "      <td>0.548559</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.347455</td>\n",
       "      <td>0.262559</td>\n",
       "      <td>0.153083</td>\n",
       "      <td>0.301671</td>\n",
       "      <td>0.536042</td>\n",
       "      <td>0.299819</td>\n",
       "      <td>0.147712</td>\n",
       "      <td>0.213025</td>\n",
       "      <td>0.242467</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.244139</td>\n",
       "      <td>-0.379522</td>\n",
       "      <td>-0.259928</td>\n",
       "      <td>-0.284804</td>\n",
       "      <td>-0.290708</td>\n",
       "      <td>-0.353717</td>\n",
       "      <td>-0.344595</td>\n",
       "      <td>-0.339721</td>\n",
       "      <td>-0.342710</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.311753</td>\n",
       "      <td>0.386325</td>\n",
       "      <td>0.374023</td>\n",
       "      <td>0.478871</td>\n",
       "      <td>0.264240</td>\n",
       "      <td>0.490883</td>\n",
       "      <td>0.451172</td>\n",
       "      <td>0.508316</td>\n",
       "      <td>0.440429</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.152357</td>\n",
       "      <td>-0.070179</td>\n",
       "      <td>-0.163960</td>\n",
       "      <td>-0.149150</td>\n",
       "      <td>-0.164307</td>\n",
       "      <td>-0.157344</td>\n",
       "      <td>-0.184704</td>\n",
       "      <td>-0.093335</td>\n",
       "      <td>-0.162654</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.871074</td>\n",
       "      <td>0.696823</td>\n",
       "      <td>0.712239</td>\n",
       "      <td>0.651063</td>\n",
       "      <td>0.681810</td>\n",
       "      <td>0.701787</td>\n",
       "      <td>0.702683</td>\n",
       "      <td>0.904438</td>\n",
       "      <td>0.866531</td>\n",
       "      <td>...</td>\n",
       "      <td>0.205045</td>\n",
       "      <td>0.174488</td>\n",
       "      <td>0.266517</td>\n",
       "      <td>0.201796</td>\n",
       "      <td>0.222621</td>\n",
       "      <td>0.233738</td>\n",
       "      <td>0.261191</td>\n",
       "      <td>0.225162</td>\n",
       "      <td>0.194357</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>1995</td>\n",
       "      <td>0.385140</td>\n",
       "      <td>0.167128</td>\n",
       "      <td>0.111940</td>\n",
       "      <td>0.280566</td>\n",
       "      <td>0.207912</td>\n",
       "      <td>0.368389</td>\n",
       "      <td>0.272384</td>\n",
       "      <td>0.397744</td>\n",
       "      <td>0.158749</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.386169</td>\n",
       "      <td>-0.433405</td>\n",
       "      <td>-0.413225</td>\n",
       "      <td>-0.383125</td>\n",
       "      <td>-0.387552</td>\n",
       "      <td>-0.495238</td>\n",
       "      <td>-0.385498</td>\n",
       "      <td>-0.379884</td>\n",
       "      <td>-0.410556</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>1996</td>\n",
       "      <td>0.977430</td>\n",
       "      <td>0.778175</td>\n",
       "      <td>0.792142</td>\n",
       "      <td>0.931308</td>\n",
       "      <td>0.927704</td>\n",
       "      <td>0.949077</td>\n",
       "      <td>0.953119</td>\n",
       "      <td>0.973709</td>\n",
       "      <td>0.964726</td>\n",
       "      <td>...</td>\n",
       "      <td>0.545376</td>\n",
       "      <td>0.491098</td>\n",
       "      <td>0.511091</td>\n",
       "      <td>0.424792</td>\n",
       "      <td>0.492508</td>\n",
       "      <td>0.459950</td>\n",
       "      <td>0.532656</td>\n",
       "      <td>0.495004</td>\n",
       "      <td>0.510312</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>1997</td>\n",
       "      <td>0.063835</td>\n",
       "      <td>0.073194</td>\n",
       "      <td>0.050260</td>\n",
       "      <td>0.084390</td>\n",
       "      <td>0.145980</td>\n",
       "      <td>0.108324</td>\n",
       "      <td>0.075576</td>\n",
       "      <td>0.117494</td>\n",
       "      <td>0.114896</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.557687</td>\n",
       "      <td>-0.552244</td>\n",
       "      <td>-0.545294</td>\n",
       "      <td>-0.537657</td>\n",
       "      <td>-0.570403</td>\n",
       "      <td>-0.489198</td>\n",
       "      <td>-0.549688</td>\n",
       "      <td>-0.511380</td>\n",
       "      <td>-0.517350</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>1998</td>\n",
       "      <td>0.591777</td>\n",
       "      <td>0.341324</td>\n",
       "      <td>0.204929</td>\n",
       "      <td>0.361646</td>\n",
       "      <td>0.286397</td>\n",
       "      <td>0.459233</td>\n",
       "      <td>0.217524</td>\n",
       "      <td>0.286685</td>\n",
       "      <td>0.335422</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.110476</td>\n",
       "      <td>-0.181320</td>\n",
       "      <td>-0.163894</td>\n",
       "      <td>-0.037015</td>\n",
       "      <td>-0.200439</td>\n",
       "      <td>-0.166484</td>\n",
       "      <td>-0.134078</td>\n",
       "      <td>-0.182023</td>\n",
       "      <td>-0.086352</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>1999</td>\n",
       "      <td>0.636446</td>\n",
       "      <td>0.706498</td>\n",
       "      <td>0.703063</td>\n",
       "      <td>0.831876</td>\n",
       "      <td>0.648540</td>\n",
       "      <td>0.913073</td>\n",
       "      <td>0.843899</td>\n",
       "      <td>0.715805</td>\n",
       "      <td>0.716515</td>\n",
       "      <td>...</td>\n",
       "      <td>0.414637</td>\n",
       "      <td>0.380221</td>\n",
       "      <td>0.433158</td>\n",
       "      <td>0.471740</td>\n",
       "      <td>0.451815</td>\n",
       "      <td>0.511891</td>\n",
       "      <td>0.480953</td>\n",
       "      <td>0.382454</td>\n",
       "      <td>0.421005</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows × 62 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0  CNN_KFold_0  CNN_KFold_1  CNN_KFold_2  CNN_KFold_3  \\\n",
       "0              0     0.441461     0.506231     0.296314     0.605879   \n",
       "1              1     0.787726     0.694683     0.722888     0.751069   \n",
       "2              2     0.347455     0.262559     0.153083     0.301671   \n",
       "3              3     0.311753     0.386325     0.374023     0.478871   \n",
       "4              4     0.871074     0.696823     0.712239     0.651063   \n",
       "...          ...          ...          ...          ...          ...   \n",
       "1995        1995     0.385140     0.167128     0.111940     0.280566   \n",
       "1996        1996     0.977430     0.778175     0.792142     0.931308   \n",
       "1997        1997     0.063835     0.073194     0.050260     0.084390   \n",
       "1998        1998     0.591777     0.341324     0.204929     0.361646   \n",
       "1999        1999     0.636446     0.706498     0.703063     0.831876   \n",
       "\n",
       "      CNN_KFold_4  CNN_KFold_5  CNN_KFold_6  CNN_KFold_7  CNN_KFold_8  ...  \\\n",
       "0        0.399027     0.408772     0.738035     0.513551     0.386520  ...   \n",
       "1        0.750623     0.796548     0.744546     0.698297     0.729899  ...   \n",
       "2        0.536042     0.299819     0.147712     0.213025     0.242467  ...   \n",
       "3        0.264240     0.490883     0.451172     0.508316     0.440429  ...   \n",
       "4        0.681810     0.701787     0.702683     0.904438     0.866531  ...   \n",
       "...           ...          ...          ...          ...          ...  ...   \n",
       "1995     0.207912     0.368389     0.272384     0.397744     0.158749  ...   \n",
       "1996     0.927704     0.949077     0.953119     0.973709     0.964726  ...   \n",
       "1997     0.145980     0.108324     0.075576     0.117494     0.114896  ...   \n",
       "1998     0.286397     0.459233     0.217524     0.286685     0.335422  ...   \n",
       "1999     0.648540     0.913073     0.843899     0.715805     0.716515  ...   \n",
       "\n",
       "      SVM_KFold_1  SVM_KFold_2  SVM_KFold_3  SVM_KFold_4  SVM_KFold_5  \\\n",
       "0        0.174909     0.164790     0.097380     0.081283     0.141068   \n",
       "1        0.558885     0.477677     0.474592     0.500053     0.522074   \n",
       "2       -0.244139    -0.379522    -0.259928    -0.284804    -0.290708   \n",
       "3       -0.152357    -0.070179    -0.163960    -0.149150    -0.164307   \n",
       "4        0.205045     0.174488     0.266517     0.201796     0.222621   \n",
       "...           ...          ...          ...          ...          ...   \n",
       "1995    -0.386169    -0.433405    -0.413225    -0.383125    -0.387552   \n",
       "1996     0.545376     0.491098     0.511091     0.424792     0.492508   \n",
       "1997    -0.557687    -0.552244    -0.545294    -0.537657    -0.570403   \n",
       "1998    -0.110476    -0.181320    -0.163894    -0.037015    -0.200439   \n",
       "1999     0.414637     0.380221     0.433158     0.471740     0.451815   \n",
       "\n",
       "      SVM_KFold_6  SVM_KFold_7  SVM_KFold_8  SVM_KFold_9  True  \n",
       "0        0.109408     0.182129     0.158753     0.095854   1.0  \n",
       "1        0.498110     0.527881     0.445533     0.548559   1.0  \n",
       "2       -0.353717    -0.344595    -0.339721    -0.342710   1.0  \n",
       "3       -0.157344    -0.184704    -0.093335    -0.162654   1.0  \n",
       "4        0.233738     0.261191     0.225162     0.194357   1.0  \n",
       "...           ...          ...          ...          ...   ...  \n",
       "1995    -0.495238    -0.385498    -0.379884    -0.410556   0.0  \n",
       "1996     0.459950     0.532656     0.495004     0.510312   0.0  \n",
       "1997    -0.489198    -0.549688    -0.511380    -0.517350   0.0  \n",
       "1998    -0.166484    -0.134078    -0.182023    -0.086352   0.0  \n",
       "1999     0.511891     0.480953     0.382454     0.421005   0.0  \n",
       "\n",
       "[2000 rows x 62 columns]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(\"Result/com_result_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cef027a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:BERT]",
   "language": "python",
   "name": "conda-env-BERT-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
