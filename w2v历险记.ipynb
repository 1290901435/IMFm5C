{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10013bff",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-01T12:28:51.444144Z",
     "start_time": "2023-06-01T12:28:49.224897Z"
    }
   },
   "outputs": [],
   "source": [
    "from features import ensembleFeature\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from torch import nn\n",
    "import re\n",
    "from sklearn.utils import shuffle\n",
    "from utils import metricsCal\n",
    "from torch.utils.data import DataLoader,TensorDataset\n",
    "import math\n",
    "import sys\n",
    "import copy\n",
    "import pickle\n",
    "from torch.autograd import Variable\n",
    "from sklearn.model_selection import KFold\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "from LSTM_Attention import SimpleModel as Model\n",
    "from matplotlib import cm\n",
    "from matplotlib.ticker import LinearLocator\n",
    "import csv\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "import itertools\n",
    "import argparse\n",
    "import os,sys,re\n",
    "import numpy as np \n",
    "from Bio import SeqIO\n",
    "import pickle as pkl\n",
    "import scipy.io as sio\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c4c47175",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-01T12:28:52.672632Z",
     "start_time": "2023-06-01T12:28:51.478343Z"
    }
   },
   "outputs": [],
   "source": [
    "def fa_seq(filepath):\n",
    "    f = open(filepath,'r')\n",
    "    x = []\n",
    "    for i in f:\n",
    "        x.append(i)\n",
    "    seq = []\n",
    "    for i in range(len(x)):\n",
    "        if i%2==1:\n",
    "            seq.append(x[i][:-1])\n",
    "    return seq\n",
    "\n",
    "def Binary(sequences):\n",
    "    AA = 'ACGU'\n",
    "    binary_feature = []\n",
    "    for seq in sequences:\n",
    "        # seq=str(seq)[2:23]\n",
    "        binary = []\n",
    "        for aa in seq:\n",
    "            binary_one = []\n",
    "            for aa1 in AA:\n",
    "                tag = 1 if aa == aa1 else 0\n",
    "                binary_one.append(tag)\n",
    "            #binary.append(binary_one)\n",
    "            binary.append(binary_one)\n",
    "        binary_feature.append(binary)\n",
    "    return np.array(binary_feature)\n",
    "\n",
    "train_seq = fa_seq(\"data/Mouse/mouse_train.fasta\")\n",
    "trainData = Binary(train_seq)\n",
    "trainLabel = np.append(np.ones(int(len(trainData)/2)),np.zeros(int(len(trainData)-len(trainData)/2)),axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "65bf618e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-01T12:29:01.872355Z",
     "start_time": "2023-06-01T12:28:52.721887Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/anaconda3/envs/BERT/lib/python3.6/site-packages/ipykernel_launcher.py:21: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:201.)\n"
     ]
    }
   ],
   "source": [
    "def fa_seq(filepath):\n",
    "    f = open(filepath,'r')\n",
    "    x = []\n",
    "    for i in f:\n",
    "        x.append(i)\n",
    "    seq = []\n",
    "    for i in range(len(x)):\n",
    "        if i%2==1:\n",
    "            seq.append(x[i][:-1])\n",
    "    return seq\n",
    "\n",
    "class EmbeddingSeq(nn.Module):\n",
    "    def __init__(self,weight_dict_path):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            weight_dict_path: path of pre-trained embeddings of RNA/dictionary\n",
    "        \"\"\"\n",
    "        super(EmbeddingSeq,self).__init__()\n",
    "        weight_dict = pickle.load(open(weight_dict_path,'rb'))\n",
    "\n",
    "        weights = torch.FloatTensor(list(weight_dict.values())).cuda()\n",
    "        num_embeddings = len(list(weight_dict.keys()))\n",
    "        embedding_dim = 300\n",
    "\n",
    "        self.embedding = nn.Embedding(num_embeddings=num_embeddings,embedding_dim=embedding_dim)\n",
    "        self.embedding.weight = nn.Parameter(weights)\n",
    "        self.embedding.weight.requires_grad = False\n",
    "\n",
    "    def forward(self,x):\n",
    "\n",
    "        out = self.embedding(x.type(torch.cuda.LongTensor))\n",
    "\n",
    "        return out\n",
    "\n",
    "def seq2index(seqs,my_dict,window=3,save_data=False):\n",
    "    \"\"\"\n",
    "    Convert single RNA sequences to k-mers representation.\n",
    "        Inputs: ['ACAUG','CAACC',...] of equal length RNA seqs\n",
    "        Example: 'ACAUG' ----> [ACA,CAU,AUG] ---->[21,34,31]\n",
    "    \"\"\"\n",
    "\n",
    "    num_samples = len(seqs)\n",
    "    temp = []\n",
    "    for k in range(num_samples):\n",
    "        length = len(seqs[k])\n",
    "        seqs_kmers = [seqs[k][i:i+window] for i in range(0,length-window+1)]\n",
    "        temp.append(seqs_kmers)\n",
    "\n",
    "\n",
    "    seq_kmers = pd.DataFrame(data = np.concatenate(temp,axis=0))\n",
    "\n",
    "    # load pretained word2vec embeddings\n",
    "\n",
    "    word2index = word2index_(my_dict)\n",
    "\n",
    "    seq_kmers_index = seq_kmers.applymap(lambda x: mapfun(x,my_dict))\n",
    "\n",
    "\n",
    "    return seq_kmers_index.to_numpy()\n",
    "def word2index_(my_dict):\n",
    "    word2index = dict()\n",
    "    for index, ele in enumerate(list(my_dict.keys())):\n",
    "        word2index[ele] = index\n",
    "\n",
    "    return word2index\n",
    "\n",
    "def mapfun(x,my_dict):\n",
    "    if x not in list(my_dict.keys()):\n",
    "        return None\n",
    "    else:\n",
    "        return word2index_(my_dict)[x]\n",
    "    \n",
    "embed = EmbeddingSeq('features/embeddings_12RM.pkl')\n",
    "embeddings_dict = pickle.load(open('features/embeddings_12RM.pkl','rb'))\n",
    "train_seq = fa_seq(\"data/Mouse/mouse_train.fasta\")\n",
    "trainLabel = np.append(np.ones(int(len(train_seq)/2)),np.zeros(int(len(train_seq)-len(train_seq)/2)),axis=0)\n",
    "seq_T = []\n",
    "for i in train_seq:\n",
    "    seq_T.append(i.replace('U','T'))\n",
    "x = seq2index(seq_T,embeddings_dict)\n",
    "seqs_kmers_index = torch.transpose(torch.from_numpy(x),0,1)\n",
    "trainData = np.array(embed(seqs_kmers_index).view(len(train_seq),-1,300).cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dccf1e02",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-01T12:29:03.464333Z",
     "start_time": "2023-06-01T12:29:03.435134Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def attention(query, key, value, mask=None, dropout=None):  # q,k,v: [batch, h, seq_len, d_k]\n",
    "    d_k = query.size(-1)  # query的维度\n",
    "    scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)  # 打分机制 [batch, h, seq_len, seq_len]\n",
    "    p_atten = F.softmax(scores, dim=-1)  # 对最后一个维度归一化得分, [batch, h, seq_len, seq_len]\n",
    "    if dropout is not None:\n",
    "        p_atten = dropout(p_atten)\n",
    "    return torch.matmul(p_atten, value), p_atten  # [batch, h, seq_len, d_k] 作矩阵的乘法\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, dim1, dim2, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "\n",
    "        #if dim % 2 != 0:\n",
    "        #    raise ValueError(\"Cannot use sin/cos positional encoding with \"\n",
    "        #                     \"odd dim (got dim={:d})\".format(dim))\n",
    "\n",
    "        \"\"\"\n",
    "        构建位置编码pe\n",
    "        pe公式为：\n",
    "        PE(pos,2i/2i+1) = sin/cos(pos/10000^{2i/d_{model}})\n",
    "        \"\"\"\n",
    "        pe = torch.zeros(max_len, dim2)  # max_len 是解码器生成句子的最长的长度，假设是 10\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        div_term0 = torch.exp((torch.arange(0, dim2, 2, dtype=torch.float) * -(math.log(10000.0) / dim2)))\n",
    "        div_term1 = torch.exp((torch.arange(1, dim2, 2, dtype=torch.float) * -(math.log(10000.0) / dim2)))\n",
    "        \n",
    "        pe[:, 0::2] = torch.sin(position.float() * div_term0)\n",
    "        pe[:, 1::2] = torch.cos(position.float() * div_term1)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "        #self.drop_out = nn.Dropout(p=dropout)\n",
    "        self.dim2 = dim2\n",
    "        self.bm1 = nn.BatchNorm1d(dim1,eps=1e-05)\n",
    "\n",
    "    def forward(self, emb, step=None):\n",
    "        emb = emb * math.sqrt(self.dim2)\n",
    "        if step is None:\n",
    "            emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
    "        else:\n",
    "            emb = emb + self.pe[step]\n",
    "        #emb = self.drop_out(emb)\n",
    "        emb = self.bm1(emb.to(torch.float32))\n",
    "        return emb\n",
    "    \n",
    "def clones(module, N):  #定义clones方法\n",
    "    return nn.ModuleList([copy.deepcopy(module)\n",
    "                          for _ in range(N)])  #让原来变量不影响,且克隆module N次\n",
    "\n",
    "class SelfAttention(nn.Module):  #多头注意力机制\n",
    "\n",
    "    def __init__(self,embedding_dim, dropout=0.1):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.linears = clones(nn.Linear(embedding_dim, embedding_dim), 4)  #克隆四份Linear网络层\n",
    "        self.dropout = nn.Dropout(p=dropout)  #定义Dropout层\n",
    "\n",
    "    def forward(self,query,key,value,mask=None):  # q,k,v: [batch, seq_len, embedding_dim]\n",
    "        nbatches = query.shape[0]  #批数量\n",
    "        query, key, value = [\n",
    "            l(x) for l, x in zip(self.linears,\n",
    "                            (query.to(torch.float32),\n",
    "                             key.to(torch.float32),\n",
    "                             value.to(torch.float32)))\n",
    "        ]  #获取zip的query,key,value权重矩阵\n",
    "        attn, p_atten = attention(query,key,value,mask=mask,dropout=self.dropout)\n",
    "        out = self.linears[-1](attn)  #得到最后一层线性层的输出\n",
    "        return out,p_atten  #返回out结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7839ea0c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-01T12:29:24.054974Z",
     "start_time": "2023-06-01T12:29:24.029953Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "from utils import metricsCal\n",
    "from torch.utils.data import DataLoader,TensorDataset\n",
    "import copy\n",
    "from torch.autograd import Variable\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "def train(model,data,label,epoch,train_device,model_dir,batch_size):\n",
    "    if os.path.exists(model_dir+'model.pt'):\n",
    "        model_train = torch.load(model_dir+'/model.pt')\n",
    "    else:\n",
    "        model_train = model\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = torch.optim.Adam(model_train.parameters(),lr=0.003)  #改变学习率\n",
    "    dataX = torch.Tensor(data).clone().detach()\n",
    "    label = torch.Tensor(label).clone().detach()\n",
    "    train_data = TensorDataset(dataX, label)\n",
    "    train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
    "    running_loss = 0.0\n",
    "    model_train = model_train.to(train_device)\n",
    "    for batch_idx,data in enumerate(train_loader,0):\n",
    "        inputs,target = data\n",
    "        #inputs = inputs.reshape(inputs.shape[0], 1, inputs.shape[1])\n",
    "        inputs = inputs.to(train_device)\n",
    "        target = target.to(train_device)\n",
    "        target = target.reshape(target.shape[0],1)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model_train(inputs)\n",
    "        loss = criterion(outputs,target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        #print(type(loss))\n",
    "        running_loss += loss.item()\n",
    "        if batch_idx == len(dataX)//batch_size:\n",
    "            #print('[%d, %5d] epoch loss: %.3f' %(epoch+1,batch_idx+1,running_loss))\n",
    "            print(running_loss)\n",
    "    save_model(model_train,model_dir)\n",
    "    model_train = torch.load(model_dir+'/model.pt')\n",
    "    #device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")  # 选择设备\n",
    "    th,_,_,_,_,_,_,_,_,_ = metricsCal.evaluate(model_train,train_loader,train_device)\n",
    "    return running_loss,th\n",
    "\n",
    "def test(data,label,best_auc,test_device,model_dir,batch_size,th):\n",
    "    model_test = load_model(model_dir)\n",
    "    #model_test.eval()\n",
    "    #model_test.to(test_device)\n",
    "    data = torch.Tensor(data).clone().detach()#torch.Tensor(data)\n",
    "    label = torch.Tensor(label).clone().detach()#.requires_grad_(True)torch.Tensor(label)\n",
    "    test_data = TensorDataset(data,label)\n",
    "    test_loader = DataLoader(test_data, shuffle=False, batch_size=batch_size)\n",
    "    #device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")  # 选择设备\n",
    "    _,_,_,_,Sen,Spe, Acc, mcc, AUC = metricsCal.evaluate(model_test,test_loader,test_device,False,th)\n",
    "    \n",
    "    print('Accuracy on test set: %d %%' %Acc)\n",
    "    print('Sensitivity on test set: %d %%' %Sen)\n",
    "    print('Speciality on test set: %d %%' %Spe)\n",
    "    print('MCC on test set: %.3f' %mcc)\n",
    "    print('auc on test set: %.3f' %AUC)\n",
    "    if(AUC > best_auc):\n",
    "        torch.save(model_test,model_dir+'model_best.pt')\n",
    "    return Sen,Spe,Acc, mcc, AUC\n",
    "\n",
    "def independTest(data,label,test_device,model_dir,batch_size,th):\n",
    "    model_test = load_bestModel(model_dir)\n",
    "    data = torch.Tensor(data).clone().detach()#torch.Tensor(data)\n",
    "    label = torch.Tensor(label).clone().detach()#.requires_grad_(True)torch.Tensor(label)\n",
    "    test_data = TensorDataset(data,label)\n",
    "    test_loader = DataLoader(test_data, shuffle=False, batch_size=batch_size)\n",
    "    #device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")  # 选择设备\n",
    "    _,_,_,_,Sen,Spe, Acc, mcc, AUC = metricsCal.evaluate(model_test,test_loader,test_device,False,th)\n",
    "    print(Acc,mcc,AUC)\n",
    "    print('Accuracy on test set: %d %%' %Acc)\n",
    "    print('Sensitivity on test set: %d %%' %Sen)\n",
    "    print('Speciality on test set: %d %%' %Spe)\n",
    "    print('MCC on test set: %.3f' %mcc)\n",
    "    print('auc on test set: %.3f' %AUC)\n",
    "    return Acc, mcc, AUC\n",
    "\n",
    "def load_model(model_dir):\n",
    "    if os.path.exists(model_dir+'model.pt'):\n",
    "        model_load = torch.load(model_dir+'model.pt')\n",
    "    return model_load\n",
    "\n",
    "def save_model(model_save,model_dir):\n",
    "    if not os.path.exists(model_dir):\n",
    "        os.makedirs(model_dir)\n",
    "    torch.save(model_save, model_dir+'model.pt')\n",
    "\n",
    "def load_bestModel(model_dir):\n",
    "    if os.path.exists(model_dir+'model_best.pt'):\n",
    "        model_load = torch.load(model_dir+'model_best.pt')#,map_location='cuda:0')\n",
    "    return model_load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e31dc1c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-01T12:29:06.825298Z",
     "start_time": "2023-06-01T12:29:06.647772Z"
    }
   },
   "outputs": [],
   "source": [
    "#CNN\n",
    "class Model_CNN(nn.Module):\n",
    "    def __init__(self, dim1, dim2, dropout=0.1):\n",
    "        super(Model_CNN, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(1,3,kernel_size=5,stride=2,padding=2)\n",
    "        self.conv2 = nn.Conv2d(3,3,kernel_size=5,stride=2,padding=2)\n",
    "        self.conv3 = nn.Conv2d(3,3,kernel_size=5,stride=2,padding=2)\n",
    "        \n",
    "        k1 = int((dim1+2*2-5)/2)+1\n",
    "        k2 = int((dim2+2*2-5)/2)+1\n",
    "        k1 = int((k1+2*2-5)/2)+1\n",
    "        k2 = int((k2+2*2-5)/2)+1\n",
    "        k1 = int((k1+2*2-5)/2)+1\n",
    "        k2 = int((k2+2*2-5)/2)+1\n",
    "        \n",
    "        self.fn1 = nn.Linear(3*k1*k2,128)\n",
    "        self.fn2 = nn.Linear(128,1)\n",
    "        self.ac = nn.Sigmoid()\n",
    "        self.bm1 = nn.BatchNorm1d(dim1)\n",
    "        \n",
    "        self.relu = nn.ReLU()#inplace=True)\n",
    "        self.drop = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x1 = x.contiguous().view(-1,1,x.shape[1],x.shape[2])\n",
    "        x1 = self.conv1(x1)\n",
    "        x1 = self.relu(x1)\n",
    "        x1 = self.conv2(x1)\n",
    "        x1 = self.relu(x1)\n",
    "        x1 = self.conv3(x1)\n",
    "        x1 = self.relu(x1)\n",
    "        x1 = x1.contiguous().view(x1.shape[0],-1)\n",
    "        x1 = self.fn1(x1)\n",
    "        x1 = self.relu(x1)\n",
    "        x1 = self.drop(x1)\n",
    "        x1 = self.fn2(x1)\n",
    "        out = self.ac(x1)\n",
    "        return out\n",
    "\n",
    "#LSTM\n",
    "class Model_LSTM(nn.Module):\n",
    "    def __init__(self, dim1, dim2, dropout=0.1):\n",
    "        super(Model_LSTM, self).__init__()\n",
    "        self.posi = PositionalEncoding(dim1,dim2,dropout)\n",
    "        self.lstm = nn.LSTM(input_size=dim2,hidden_size=dim2,batch_first=True,bidirectional=False)\n",
    "        self.bm1 = nn.BatchNorm1d(dim1)\n",
    "        self.conv1 = nn.Conv2d(1,1,kernel_size=5,stride=2,padding=2)\n",
    "        k1 = int((dim1+2*2-5)/2)+1\n",
    "        k2 = int((dim2+2*2-5)/2)+1\n",
    "        self.fn1 = nn.Linear(1*k1*k2,128)\n",
    "        #self.fn1 = nn.Linear(dim1*dim2,128)\n",
    "        self.fn2 = nn.Linear(128,1)\n",
    "        self.ac = nn.Sigmoid()\n",
    "        \n",
    "        self.relu = nn.ReLU()#inplace=True)\n",
    "        self.drop = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #所有输入，如果只有一维，需要变成1*n维\n",
    "        x1 = self.posi(x)\n",
    "        x1,(h_n,c_n) = self.lstm(x1)\n",
    "        x1 = self.relu(x1)\n",
    "        x1 = self.bm1(x1)\n",
    "        \n",
    "        x1 = x1.contiguous().view(-1,1,x1.shape[1],x1.shape[2])\n",
    "        x1 = self.conv1(x1)\n",
    "        x1 = self.relu(x1)\n",
    "        x1 = x1.contiguous().view(x1.shape[0],-1)\n",
    "        x1 = self.fn1(x1)\n",
    "        x1 = self.relu(x1)\n",
    "        x1 = self.drop(x1)\n",
    "        x1 = self.fn2(x1)\n",
    "        out = self.ac(x1)\n",
    "        return out\n",
    "\n",
    "#BiLSTM\n",
    "class Model_BiLSTM(nn.Module):\n",
    "    def __init__(self, dim1, dim2, dropout=0.1):\n",
    "        super(Model_BiLSTM, self).__init__()\n",
    "        self.posi = PositionalEncoding(dim1,dim2,dropout)\n",
    "        self.lstm = nn.LSTM(input_size=dim2,hidden_size=dim2,batch_first=True,bidirectional=True)\n",
    "        self.bm1 = nn.BatchNorm1d(dim1)\n",
    "        self.conv1 = nn.Conv2d(1,1,kernel_size=5,stride=2,padding=2)\n",
    "        k1 = int((dim1+2*2-5)/2)+1\n",
    "        k2 = int((dim2+2*2-5)/2)+1\n",
    "        self.fn1 = nn.Linear(1*k1*k2,128)\n",
    "        self.fn2 = nn.Linear(128,1)\n",
    "        self.ac = nn.Sigmoid()\n",
    "        \n",
    "        self.relu = nn.ReLU()#inplace=True)\n",
    "        self.drop = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #所有输入，如果只有一维，需要变成1*n维\n",
    "        x1 = self.posi(x)\n",
    "        x1,(h_n,c_n) = self.lstm(x1)\n",
    "        x1 = x1[:,:,0:300]+x1[:,:,300:600]\n",
    "        x1 = self.relu(x1)\n",
    "        x1 = self.bm1(x1)\n",
    "        x1 = x1.contiguous().view(-1,1,x1.shape[1],x1.shape[2])\n",
    "        x1 = self.conv1(x1)\n",
    "        x1 = self.relu(x1)\n",
    "        x1 = x1.contiguous().view(x1.shape[0],-1)\n",
    "        x1 = self.fn1(x1)\n",
    "        x1 = self.relu(x1)\n",
    "        x1 = self.drop(x1)\n",
    "        x1 = self.fn2(x1)\n",
    "        out = self.ac(x1)\n",
    "        return out\n",
    "    \n",
    "#Attention\n",
    "class Model_Attention(nn.Module):\n",
    "    def __init__(self, dim1, dim2, dropout=0.1):\n",
    "        super(Model_Attention, self).__init__()\n",
    "        self.posi = PositionalEncoding(dim1,dim2,dropout)\n",
    "        self.self_A = SelfAttention(dim2)\n",
    "        self.self_B = SelfAttention(dim1)\n",
    "        self.bm1 = nn.BatchNorm1d(dim1)\n",
    "        self.conv1 = nn.Conv2d(1,1,kernel_size=5,stride=2,padding=2)\n",
    "        k1 = int((dim1+2*2-5)/2)+1\n",
    "        k2 = int((dim2+2*2-5)/2)+1\n",
    "        self.fn1 = nn.Linear(1*k1*k2,128)\n",
    "        self.fn2 = nn.Linear(128,1)\n",
    "        self.ac = nn.Sigmoid()\n",
    "        self.bm1 = nn.BatchNorm1d(dim1)\n",
    "        \n",
    "        self.relu = nn.ReLU()#inplace=True)\n",
    "        self.drop = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #所有输入，如果只有一维，需要变成1*n维\n",
    "        x1 = self.posi(x)\n",
    "        if x1.shape[1] == 1:\n",
    "            x1 = x1.view(x1.shape[0],x1.shape[2],x1.shape[1])\n",
    "            x1,attn = self.self_B(x1,x1,x1)\n",
    "        else:\n",
    "            x1,attn = self.self_A(x1,x1,x1)\n",
    "        if x1.shape[2]==1:\n",
    "            x1 = x1.view(x1.shape[0],x1.shape[2],x1.shape[1])\n",
    "        x1 = self.bm1(x1)\n",
    "        x1 = self.relu(x1)\n",
    "        \n",
    "        x1 = x1.contiguous().view(-1,1,x1.shape[1],x1.shape[2])\n",
    "        x1 = self.conv1(x1)\n",
    "        x1 = self.relu(x1)\n",
    "        x1 = x1.contiguous().view(x1.shape[0],-1)\n",
    "        x1 = self.fn1(x1)\n",
    "        x1 = self.relu(x1)\n",
    "        x1 = self.drop(x1)\n",
    "        x1 = self.fn2(x1)\n",
    "        out = self.ac(x1)\n",
    "        return out\n",
    "\n",
    "#LSTM_Attention_CNN\n",
    "class Model_LSTM_Attention_CNN(nn.Module):\n",
    "    def __init__(self, dim1, dim2, dropout=0.1):\n",
    "        super(Model_LSTM_Attention_CNN, self).__init__()\n",
    "        self.posi = PositionalEncoding(dim1,dim2,dropout)\n",
    "        self.self_A = SelfAttention(dim2)\n",
    "        self.self_B = SelfAttention(dim1)\n",
    "        self.lstm = nn.LSTM(input_size=dim2,hidden_size=dim2,batch_first=True,bidirectional=False)\n",
    "        self.bm1 = nn.BatchNorm1d(dim1)\n",
    "        self.conv1 = nn.Conv2d(1,1,kernel_size=5,stride=2,padding=2)\n",
    "        k1 = int((dim1+2*2-5)/2)+1\n",
    "        k2 = int((dim2+2*2-5)/2)+1\n",
    "        self.fn1 = nn.Linear(1*k1*k2,128)\n",
    "        self.fn2 = nn.Linear(128,1)\n",
    "        self.ac = nn.Sigmoid()\n",
    "        self.bm1 = nn.BatchNorm1d(dim1)\n",
    "        \n",
    "        self.relu = nn.ReLU()#inplace=True)\n",
    "        self.drop = nn.Dropout(p=dropout)\n",
    "        #self.encoder = nn.Sequential()\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        #所有输入，如果只有一维，需要变成1*n维\n",
    "        x1 = self.posi(x)\n",
    "        x1,(h_n,c_n) = self.lstm(x1)\n",
    "        x1 = self.relu(x1)\n",
    "        if x1.shape[1] == 1:\n",
    "            x1 = x1.view(x1.shape[0],x1.shape[2],x1.shape[1])\n",
    "            x1,attn = self.self_B(x1,x1,x1)\n",
    "        else:\n",
    "            x1,attn = self.self_A(x1,x1,x1)\n",
    "        if x1.shape[2]==1:\n",
    "            x1 = x1.view(x1.shape[0],x1.shape[2],x1.shape[1])\n",
    "        x1 = self.bm1(x1)\n",
    "        x1 = self.relu(x1)\n",
    "        \n",
    "        x1 = x1.contiguous().view(-1,1,x1.shape[1],x1.shape[2])\n",
    "        x1 = self.conv1(x1)\n",
    "        x1 = self.relu(x1)\n",
    "        x1 = x1.contiguous().view(x1.shape[0],-1)\n",
    "        x1 = self.fn1(x1)\n",
    "        x1 = self.relu(x1)\n",
    "        x1 = self.drop(x1)\n",
    "        x1 = self.fn2(x1)\n",
    "        out = self.ac(x1)\n",
    "        return out\n",
    "    \n",
    "#BiLSTM_Attention_CNN\n",
    "class Model_BiLSTM_Attention_CNN(nn.Module):\n",
    "    def __init__(self, dim1, dim2, dropout=0.1):\n",
    "        super(Model_BiLSTM_Attention_CNN, self).__init__()\n",
    "        self.posi = PositionalEncoding(dim1,dim2,dropout)\n",
    "        self.self_A = SelfAttention(dim2)\n",
    "        self.self_B = SelfAttention(dim1)\n",
    "        self.lstm = nn.LSTM(input_size=dim2,hidden_size=dim2,batch_first=True,bidirectional=True)\n",
    "        self.bm1 = nn.BatchNorm1d(dim1)\n",
    "        self.conv1 = nn.Conv2d(1,1,kernel_size=5,stride=2,padding=2)\n",
    "        k1 = int((dim1+2*2-5)/2)+1\n",
    "        k2 = int((dim2+2*2-5)/2)+1\n",
    "        self.fn1 = nn.Linear(1*k1*k2,128)\n",
    "        self.fn2 = nn.Linear(128,1)\n",
    "        self.ac = nn.Sigmoid()\n",
    "        self.bm1 = nn.BatchNorm1d(dim1)\n",
    "        \n",
    "        self.relu = nn.ReLU()#inplace=True)\n",
    "        self.drop = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        #所有输入，如果只有一维，需要变成1*n维\n",
    "        x1 = self.posi(x)\n",
    "        x1,(h_n,c_n) = self.lstm(x1)\n",
    "        x1 = x1[:,:,0:300]+x1[:,:,300:600]\n",
    "        x1 = self.relu(x1)\n",
    "        if x1.shape[1] == 1:\n",
    "            x1 = x1.view(x1.shape[0],x1.shape[2],x1.shape[1])\n",
    "            x1,attn = self.self_B(x1,x1,x1)\n",
    "        else:\n",
    "            x1,attn = self.self_A(x1,x1,x1)\n",
    "        if x1.shape[2]==1:\n",
    "            x1 = x1.view(x1.shape[0],x1.shape[2],x1.shape[1])\n",
    "        x1 = self.bm1(x1)\n",
    "        x1 = self.relu(x1)\n",
    "        \n",
    "        x1 = x1.contiguous().view(-1,1,x1.shape[1],x1.shape[2])\n",
    "        x1 = self.conv1(x1)\n",
    "        x1 = self.relu(x1)\n",
    "        x1 = x1.contiguous().view(x1.shape[0],-1)\n",
    "        x1 = self.fn1(x1)\n",
    "        x1 = self.relu(x1)\n",
    "        x1 = self.drop(x1)\n",
    "        x1 = self.fn2(x1)\n",
    "        out = self.ac(x1)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b8622851",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-01T12:34:03.216029Z",
     "start_time": "2023-06-01T12:29:28.123435Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/anaconda3/envs/BERT/lib/python3.6/site-packages/sklearn/utils/validation.py:72: FutureWarning: Pass shuffle=True, random_state=0 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error\n",
      "  \"will result in an error\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23.844664692878723\n",
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.115\n",
      "auc on test set: 0.595\n",
      "23.986645698547363\n",
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.118\n",
      "auc on test set: 0.587\n",
      "23.188163340091705\n",
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.207\n",
      "auc on test set: 0.636\n",
      "23.086291253566742\n",
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.068\n",
      "auc on test set: 0.531\n",
      "23.013870298862457\n",
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.055\n",
      "auc on test set: 0.477\n",
      "23.026685774326324\n",
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: -0.006\n",
      "auc on test set: 0.496\n",
      "23.008689045906067\n",
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 1 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.000\n",
      "auc on test set: 0.471\n",
      "22.941567301750183\n",
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: -0.007\n",
      "auc on test set: 0.497\n",
      "22.959461987018585\n",
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 1 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.000\n",
      "auc on test set: 0.513\n",
      "22.971606016159058\n",
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 1 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.000\n",
      "auc on test set: 0.499\n",
      "22.98993682861328\n",
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 1 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.000\n",
      "auc on test set: 0.484\n",
      "22.9306703209877\n",
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 1 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.000\n",
      "auc on test set: 0.511\n",
      "22.980277359485626\n",
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: -0.042\n",
      "auc on test set: 0.482\n",
      "22.99207365512848\n",
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: -0.036\n",
      "auc on test set: 0.486\n",
      "22.953854382038116\n",
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: -0.033\n",
      "auc on test set: 0.496\n",
      "22.97100019454956\n",
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: -0.039\n",
      "auc on test set: 0.480\n",
      "22.94539338350296\n",
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: -0.069\n",
      "auc on test set: 0.460\n",
      "22.968583405017853\n",
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 1 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.000\n",
      "auc on test set: 0.490\n",
      "22.928506731987\n",
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.078\n",
      "auc on test set: 0.540\n",
      "22.947045922279358\n",
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.084\n",
      "auc on test set: 0.550\n",
      "23.024643182754517\n",
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 1 %\n",
      "MCC on test set: 0.000\n",
      "auc on test set: 0.406\n",
      "22.94083935022354\n",
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: -0.008\n",
      "auc on test set: 0.432\n",
      "22.970940470695496\n",
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: -0.015\n",
      "auc on test set: 0.500\n",
      "22.937357127666473\n",
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.024\n",
      "auc on test set: 0.476\n",
      "22.918455839157104\n",
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.019\n",
      "auc on test set: 0.504\n",
      "22.92881852388382\n",
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.036\n",
      "auc on test set: 0.520\n",
      "22.899528801441193\n",
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.003\n",
      "auc on test set: 0.504\n",
      "22.904098749160767\n",
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.010\n",
      "auc on test set: 0.504\n",
      "22.982345521450043\n",
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 1 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.000\n",
      "auc on test set: 0.508\n",
      "24.363585770130157\n",
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.062\n",
      "auc on test set: 0.506\n",
      "23.14559930562973\n",
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.254\n",
      "auc on test set: 0.685\n",
      "23.24142813682556\n",
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.290\n",
      "auc on test set: 0.695\n",
      "22.618599712848663\n",
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.320\n",
      "auc on test set: 0.706\n",
      "22.572296500205994\n",
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.336\n",
      "auc on test set: 0.708\n",
      "22.400470435619354\n",
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.314\n",
      "auc on test set: 0.712\n",
      "22.205306231975555\n",
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.365\n",
      "auc on test set: 0.697\n",
      "22.42882615327835\n",
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.359\n",
      "auc on test set: 0.682\n",
      "22.477929770946503\n",
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: -0.033\n",
      "auc on test set: 0.426\n",
      "22.935133039951324\n",
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.173\n",
      "auc on test set: 0.517\n",
      "22.92614310979843\n",
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.196\n",
      "auc on test set: 0.632\n",
      "22.959639072418213\n",
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.093\n",
      "auc on test set: 0.550\n",
      "22.9298477768898\n",
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 1 %\n",
      "MCC on test set: 0.033\n",
      "auc on test set: 0.425\n",
      "22.995141565799713\n",
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.066\n",
      "auc on test set: 0.547\n",
      "22.959298133850098\n",
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.131\n",
      "auc on test set: 0.583\n",
      "22.940072774887085\n",
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.107\n",
      "auc on test set: 0.598\n",
      "22.98174500465393\n",
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.191\n",
      "auc on test set: 0.627\n",
      "22.921760618686676\n",
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: -0.037\n",
      "auc on test set: 0.484\n",
      "22.942153215408325\n",
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.105\n",
      "auc on test set: 0.585\n",
      "22.925806879997253\n",
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.060\n",
      "auc on test set: 0.510\n",
      "22.95869904756546\n",
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.127\n",
      "auc on test set: 0.563\n",
      "22.92515116930008\n",
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 1 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.048\n",
      "auc on test set: 0.484\n",
      "23.052860140800476\n",
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 1 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.034\n",
      "auc on test set: 0.406\n",
      "22.914266884326935\n",
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.154\n",
      "auc on test set: 0.570\n",
      "22.894889891147614\n",
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.081\n",
      "auc on test set: 0.515\n",
      "22.942325174808502\n",
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.128\n",
      "auc on test set: 0.586\n",
      "22.944913923740387\n",
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 1 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.000\n",
      "auc on test set: 0.413\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22.907622277736664\n",
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.070\n",
      "auc on test set: 0.511\n",
      "22.92661440372467\n",
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.195\n",
      "auc on test set: 0.642\n",
      "22.912088334560394\n",
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 1 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.000\n",
      "auc on test set: 0.500\n",
      "22.956439435482025\n",
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 0 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.178\n",
      "auc on test set: 0.588\n",
      "22.920087695121765\n",
      "Accuracy on test set: 0 %\n",
      "Sensitivity on test set: 1 %\n",
      "Speciality on test set: 0 %\n",
      "MCC on test set: 0.000\n",
      "auc on test set: 0.500\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-b2bdc378b832>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0mrunningLoss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m         \u001b[0msen\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mspe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmcc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauc\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbest_auc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mLoss\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrunningLoss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-ccf64610ab41>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, data, label, epoch, train_device, model_dir, batch_size)\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;31m#print(type(loss))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0mrunning_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_idx\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0;31m#print('[%d, %5d] epoch loss: %.3f' %(epoch+1,batch_idx+1,running_loss))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1."
     ]
    }
   ],
   "source": [
    "#先做交叉验证，看看多少个epoch合适\n",
    "rows = 10\n",
    "row = -1\n",
    "max_epochs = 150\n",
    "max_patience = 25\n",
    "batch_size = 256\n",
    "row_name = []\n",
    "Loss = np.zeros(rows*max_epochs).reshape(rows,max_epochs)\n",
    "Sen = np.zeros(rows*(max_epochs+1)).reshape(rows,(max_epochs+1))\n",
    "Spe = np.zeros(rows*(max_epochs+1)).reshape(rows,(max_epochs+1))\n",
    "Acc = np.zeros(rows*(max_epochs+1)).reshape(rows,(max_epochs+1))\n",
    "Mcc = np.zeros(rows*(max_epochs+1)).reshape(rows,(max_epochs+1))\n",
    "Auc = np.zeros(rows*(max_epochs+1)).reshape(rows,(max_epochs+1))\n",
    "th_All = np.zeros(rows*(max_epochs+1)).reshape(rows,(max_epochs+1))\n",
    "\n",
    "#然后全部all_in 一个模型出来\n",
    "#%env CUDA_LAUNCH_BLOCKING=1\n",
    "kf = KFold(10,True,0)\n",
    "#第三重循环\n",
    "for i,[train_index, test_index] in enumerate(kf.split(trainData)):\n",
    "    row+=1\n",
    "    row_one = \"LSTM_Attention_CNN_w2v_KFold_\" + str(i)\n",
    "    row_name.append(row_one)\n",
    "    X_train = trainData[train_index]\n",
    "    X_test = trainData[test_index]\n",
    "    Y_train = trainLabel[train_index]\n",
    "    Y_test = trainLabel[test_index]\n",
    "    best_auc = 0\n",
    "    patience = 0\n",
    "    model_dir = \"Model/Model_Trans_CNN_RuLU_w2v/KFold_\" + str(i)+\"/\"\n",
    "    model = TransCNN()\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")  #选择设备\n",
    "    model = model.to(device)\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "\n",
    "    for j in range(max_epochs):\n",
    "        runningLoss,th = train(model,X_train,Y_train,j,device,model_dir,batch_size)\n",
    "        sen,spe, acc, mcc, auc= test(X_test,Y_test,best_auc,device,model_dir,batch_size,th)\n",
    "        Loss[row][j] = runningLoss\n",
    "        th_All[row][j] = th\n",
    "        Sen[row][j] = sen\n",
    "        Spe[row][j] = spe\n",
    "        Acc[row][j] = acc\n",
    "        Mcc[row][j] = mcc\n",
    "        Auc[row][j] = auc                            \n",
    "        if auc > best_auc:\n",
    "            best_auc=auc\n",
    "            patience=0\n",
    "        else:\n",
    "            patience+=1\n",
    "            \n",
    "        if patience>max_patience and best_auc!= 0.5:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "de951182",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-01T12:38:58.964979Z",
     "start_time": "2023-06-01T12:38:58.959545Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.63551774, 0.71199597, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(Auc,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ca706ef5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-01T12:16:12.851661Z",
     "start_time": "2023-06-01T12:16:12.822663Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8048860375000648"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(pd.read_csv(\"Model/w2v_BiLSTM_Attention_CNN.csv\").iloc[-10:,2:],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "aa0b0762",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-31T10:48:34.915400Z",
     "start_time": "2023-05-31T10:48:34.908783Z"
    }
   },
   "outputs": [],
   "source": [
    "Row_name = []\n",
    "name = [\"Loss\",\"Thr\",\"Sen\",\"Spe\",\"Acc\",\"Mcc\",\"Auc\"]\n",
    "for i in range(7):\n",
    "    for j in range(10):\n",
    "        Row_name.append(name[i]+\"_KFold_\"+str(j))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "199e17f5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-01T05:49:49.303740Z",
     "start_time": "2023-06-01T05:49:49.281136Z"
    }
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(np.hstack((np.array(Row_name).reshape(-1,1),np.vstack((Loss,th_All[:,:150],Sen[:,:150],Spe[:,:150],Acc[:,:150],Mcc[:,:150],Auc[:,:150]))))).to_csv(\"Model/w2v_BiLSTM.csv\")#,header=[\"Name\",\"Loss\",\"Thr\",\"Sen\",\"Spe\",\"Acc\",\"Mcc\",\"Auc\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f059455d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#LSTM+Attn+CNN  0.811\n",
    "# array([0.8006344 , 0.81798675, 0.82836602, 0.80211214, 0.81431039,\n",
    "#        0.81716833, 0.80938502, 0.79952567, 0.82470794, 0.80080841])\n",
    "\n",
    "#CNN\n",
    "# array([0.78121371, 0.80199213, 0.80797362, 0.79136905, 0.81091276,\n",
    "#        0.81586777, 0.81027476, 0.79999711, 0.81386285, 0.79199288])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "efe7e744",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-01T09:15:23.071922Z",
     "start_time": "2023-06-01T09:15:23.068138Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba0fc64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8b299dd8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-01T12:29:08.394414Z",
     "start_time": "2023-06-01T12:29:08.371032Z"
    }
   },
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "from torch import Tensor\n",
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, max_len: int = 416):#将64改为416\n",
    "        super().__init__()\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1).unsqueeze(2) # [maxlen, 1, 1]\n",
    "        div_term = 2 * math.pi * torch.exp(torch.arange(0, d_model, 2) * (-math.log(512.0) / d_model))\n",
    "        div_term = div_term.unsqueeze(0).unsqueeze(0) # [1, 1,  d_model // 2]\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, :, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, :, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor, shape [seq_len, batch_size, embedding_dim]\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:x.size(0), :, :]\n",
    "        return x\n",
    "class TransformerEncoderLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, dim_feedforward=2048, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.self_attn = nn.MultiheadAttention(d_model, 1, dropout=dropout)\n",
    "        \n",
    "        self.linear1 = nn.Linear(d_model, dim_feedforward)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear2 = nn.Linear(dim_feedforward, d_model)\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "    def with_pos_embed(self, tensor, pos: Optional[Tensor]):\n",
    "        return tensor if pos is None else tensor + pos\n",
    "\n",
    "    def forward(self,\n",
    "                     src,\n",
    "                     src_mask: Optional[Tensor] = None,\n",
    "                     src_key_padding_mask: Optional[Tensor] = None,\n",
    "                     pos: Optional[Tensor] = None):\n",
    "        q = k = self.with_pos_embed(src, pos)\n",
    "        src2 = self.self_attn(q, k, value=src, attn_mask=src_mask,\n",
    "                              key_padding_mask=src_key_padding_mask)[0]\n",
    "        src = src + self.dropout1(src2)\n",
    "        src = self.norm1(src)\n",
    "        src2 = self.linear2(self.dropout(self.activation(self.linear1(src))))#######MLP############\n",
    "        src = src + self.dropout2(src2)\n",
    "        src = self.norm2(src)\n",
    "        \n",
    "        return src\n",
    "\n",
    "class TransCNN(nn.Module):\n",
    "    def __init__(self, input_dim=39, ouput_dim=64, dropout=0.1):\n",
    "        super(TransCNN, self).__init__()\n",
    "        self.ouput_dim = ouput_dim\n",
    "\n",
    "        self.conv1 = nn.Conv1d(input_dim, ouput_dim, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv1d(ouput_dim, ouput_dim, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv1d(ouput_dim, ouput_dim, kernel_size=3, stride=2, padding=1)\n",
    "        \n",
    "        self.pos_embed = PositionalEncoding(ouput_dim)\n",
    "        self.query_embed = nn.Embedding(1, ouput_dim)\n",
    "\n",
    "        self.encoders = nn.ModuleList([\n",
    "            TransformerEncoderLayer(ouput_dim, dim_feedforward=4*ouput_dim, dropout=dropout)\n",
    "            for _ in range(3)\n",
    "        ])\n",
    "\n",
    "        self.linear_1 = nn.Linear(64, 4 * ouput_dim)\n",
    "        self.linear_2 = nn.Linear(4 * ouput_dim, ouput_dim)\n",
    "        self.linear_3 = nn.Linear(ouput_dim, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.predictor = nn.Sequential(self.linear_1, self.relu, self.linear_2, self.relu, self.linear_3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.relu(self.conv2(x))\n",
    "        x = self.relu(self.conv3(x))\n",
    "        \n",
    "        x = x.permute(2, 0, 1) # [B, C, T] -> [T, B, C]\n",
    "        \n",
    "\n",
    "        query_embed = self.query_embed.weight.unsqueeze(1).repeat(1, x.shape[1], 1) # [1, B, C]\n",
    "        #print(query_embed.shape)\n",
    "        x = torch.cat([query_embed, x], dim=0) # [T + 1, B, C]\n",
    "        \n",
    "        pos = self.pos_embed(x)\n",
    "        for encoder in self.encoders:\n",
    "            x = encoder(x, pos=pos)\n",
    "        out = self.predictor(x[0])\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "92d5586e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-01T12:17:00.244055Z",
     "start_time": "2023-06-01T12:17:00.233397Z"
    }
   },
   "outputs": [],
   "source": [
    "net = TransCNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2b8b23a6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-01T12:17:00.723975Z",
     "start_time": "2023-06-01T12:17:00.592478Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 1])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net(torch.Tensor(trainData[0:100])).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "466c5f25",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-01T12:17:06.111075Z",
     "start_time": "2023-06-01T12:17:06.089274Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# 选择要使用的设备，如cuda:0\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 创建一个Tensor对象并将其移动到设备上\n",
    "tensor = torch.Tensor([0]).to(device)\n",
    "\n",
    "# 将Tensor对象分配给另一个新的变量以发起销毁操作\n",
    "del tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf3ecbe2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:BERT]",
   "language": "python",
   "name": "conda-env-BERT-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
