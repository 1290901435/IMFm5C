{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "286469ec",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-03T08:24:30.025444Z",
     "start_time": "2023-06-03T08:24:27.867384Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from features import ensembleFeature\n",
    "import itertools\n",
    "from utils import metricsCal\n",
    "from thundersvm import SVC\n",
    "from sklearn.model_selection import KFold\n",
    "import pymrmr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bfd41a00",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-03T08:24:32.226496Z",
     "start_time": "2023-06-03T08:24:32.217718Z"
    }
   },
   "outputs": [],
   "source": [
    "##将序列转化为词向量用到的函数\n",
    "def fa_seq(filepath):\n",
    "    f = open(filepath,'r')\n",
    "    x = []\n",
    "    for i in f:\n",
    "        x.append(i)\n",
    "    seq = []\n",
    "    for i in range(len(x)):\n",
    "        if i%2==1:\n",
    "            seq.append(x[i][:-1])\n",
    "    return seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0dc2afb8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-03T08:24:39.217330Z",
     "start_time": "2023-06-03T08:24:39.191862Z"
    }
   },
   "outputs": [],
   "source": [
    "train_seq = fa_seq(\"data/Mouse/mouse_train.fasta\")\n",
    "test_seq = fa_seq(\"data/Mouse/mouse_indep.fasta\")\n",
    "trainPos_seq = train_seq[0:int(len(train_seq)/2)]\n",
    "trainNeg_seq = train_seq[int(len(train_seq)/2):]\n",
    "testPos_seq = test_seq[0:int(len(test_seq)/2)]\n",
    "testNeg_seq = test_seq[int(len(test_seq)/2):]\n",
    "trainLabel = np.append(np.ones(int(len(train_seq)/2)),np.zeros(int(len(train_seq)-len(train_seq)/2)),axis=0)\n",
    "testLabel = np.append(np.ones(int(len(test_seq)/2)),np.zeros(int(len(test_seq)-len(test_seq)/2)),axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d84b70a9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-03T08:25:25.171961Z",
     "start_time": "2023-06-03T08:25:15.755131Z"
    }
   },
   "outputs": [],
   "source": [
    "# 值\n",
    "trainPos,trainNeg,testPos,testNeg = ensembleFeature.PSKP(trainPos_seq,trainNeg_seq,testPos_seq,testNeg_seq,1) #3代表PSTP，2代表PSDP，1代表PSNP\n",
    "trainData_PSNP = np.vstack((trainPos,trainNeg))\n",
    "testData_PSNP = np.vstack((testPos,testNeg))\n",
    "trainPos,trainNeg,testPos,testNeg = ensembleFeature.PSKP(trainPos_seq,trainNeg_seq,testPos_seq,testNeg_seq,2) #3代表PSTP，2代表PSDP，1代表PSNP\n",
    "trainData_PSDP = np.vstack((trainPos,trainNeg))\n",
    "testData_PSDP = np.vstack((testPos,testNeg))\n",
    "trainPos,trainNeg,testPos,testNeg = ensembleFeature.PSKP(trainPos_seq,trainNeg_seq,testPos_seq,testNeg_seq,3) #3代表PSTP，2代表PSDP，1代表PSNP\n",
    "trainData_PSTP = np.vstack((trainPos,trainNeg))\n",
    "testData_PSTP = np.vstack((testPos,testNeg))\n",
    "trainData_PSKP = np.hstack((trainData_PSNP,trainData_PSDP,trainData_PSTP))\n",
    "testData_PSKP = np.hstack((testData_PSNP,testData_PSDP,testData_PSTP))\n",
    "\n",
    "\n",
    "\n",
    "trainPos,trainNeg,testPos,testNeg = ensembleFeature.PSCP(trainPos_seq,trainNeg_seq,testPos_seq,testNeg_seq,0)\n",
    "trainData_PSCP_0 = np.vstack((trainPos,trainNeg))\n",
    "testData_PSCP_0 = np.vstack((testPos,testNeg))\n",
    "trainPos,trainNeg,testPos,testNeg = ensembleFeature.PSCP(trainPos_seq,trainNeg_seq,testPos_seq,testNeg_seq,1)\n",
    "trainData_PSCP_1 = np.vstack((trainPos,trainNeg))\n",
    "testData_PSCP_1 = np.vstack((testPos,testNeg))\n",
    "trainPos,trainNeg,testPos,testNeg = ensembleFeature.PSCP(trainPos_seq,trainNeg_seq,testPos_seq,testNeg_seq,2)\n",
    "trainData_PSCP_2 = np.vstack((trainPos,trainNeg))\n",
    "testData_PSCP_2 = np.vstack((testPos,testNeg))\n",
    "\n",
    "trainData_PSCP = np.hstack((trainData_PSCP_0,trainData_PSCP_1,trainData_PSCP_2))\n",
    "testData_PSCP = np.hstack((testData_PSCP_0,testData_PSCP_1,testData_PSCP_2))\n",
    "\n",
    "trainPos,trainNeg,testPos,testNeg = ensembleFeature.BPB(trainPos_seq,trainNeg_seq,testPos_seq,testNeg_seq)\n",
    "trainData_BPB = np.vstack((trainPos,trainNeg))\n",
    "testData_BPB = np.vstack((testPos,testNeg))\n",
    "\n",
    "trainData_KNN = np.array(pd.read_csv(\"features/KNN_value_train.csv\").iloc[:,1:])\n",
    "testData_KNN = np.array(pd.read_csv(\"features/KNN_value_test.csv\").iloc[:,1:])\n",
    "\n",
    "trainData_loc = np.hstack((trainData_PSKP,trainData_PSCP,trainData_BPB,trainData_KNN))\n",
    "\n",
    "testData_loc = np.hstack((testData_PSKP,testData_PSCP,testData_BPB,testData_KNN))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "81272d1b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-03T08:26:08.824850Z",
     "start_time": "2023-06-03T08:26:08.819003Z"
    }
   },
   "outputs": [],
   "source": [
    "trainData = trainData_loc.reshape(trainData_loc.shape[0],1,-1)\n",
    "testData = testData_loc.reshape(testData_loc.shape[0],1,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c8609b62",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-03T08:26:24.286002Z",
     "start_time": "2023-06-03T08:26:24.259208Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def attention(query, key, value, mask=None, dropout=None):  # q,k,v: [batch, h, seq_len, d_k]\n",
    "    d_k = query.size(-1)  # query的维度\n",
    "    scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)  # 打分机制 [batch, h, seq_len, seq_len]\n",
    "    p_atten = F.softmax(scores, dim=-1)  # 对最后一个维度归一化得分, [batch, h, seq_len, seq_len]\n",
    "    if dropout is not None:\n",
    "        p_atten = dropout(p_atten)\n",
    "    return torch.matmul(p_atten, value), p_atten  # [batch, h, seq_len, d_k] 作矩阵的乘法\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, dim1, dim2, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "\n",
    "        #if dim % 2 != 0:\n",
    "        #    raise ValueError(\"Cannot use sin/cos positional encoding with \"\n",
    "        #                     \"odd dim (got dim={:d})\".format(dim))\n",
    "\n",
    "        \"\"\"\n",
    "        构建位置编码pe\n",
    "        pe公式为：\n",
    "        PE(pos,2i/2i+1) = sin/cos(pos/10000^{2i/d_{model}})\n",
    "        \"\"\"\n",
    "        pe = torch.zeros(max_len, dim2)  # max_len 是解码器生成句子的最长的长度，假设是 10\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        div_term0 = torch.exp((torch.arange(0, dim2, 2, dtype=torch.float) * -(math.log(10000.0) / dim2)))\n",
    "        div_term1 = torch.exp((torch.arange(1, dim2, 2, dtype=torch.float) * -(math.log(10000.0) / dim2)))\n",
    "        \n",
    "        pe[:, 0::2] = torch.sin(position.float() * div_term0)\n",
    "        pe[:, 1::2] = torch.cos(position.float() * div_term1)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "        #self.drop_out = nn.Dropout(p=dropout)\n",
    "        self.dim2 = dim2\n",
    "        self.bm1 = nn.BatchNorm1d(dim1,eps=1e-05)\n",
    "\n",
    "    def forward(self, emb, step=None):\n",
    "        emb = emb * math.sqrt(self.dim2)\n",
    "        if step is None:\n",
    "            emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n",
    "        else:\n",
    "            emb = emb + self.pe[step]\n",
    "        #emb = self.drop_out(emb)\n",
    "        emb = self.bm1(emb.to(torch.float32))\n",
    "        return emb\n",
    "    \n",
    "def clones(module, N):  #定义clones方法\n",
    "    return nn.ModuleList([copy.deepcopy(module)\n",
    "                          for _ in range(N)])  #让原来变量不影响,且克隆module N次\n",
    "\n",
    "class SelfAttention(nn.Module):  #多头注意力机制\n",
    "\n",
    "    def __init__(self,embedding_dim, dropout=0.1):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.linears = clones(nn.Linear(embedding_dim, embedding_dim), 4)  #克隆四份Linear网络层\n",
    "        self.dropout = nn.Dropout(p=dropout)  #定义Dropout层\n",
    "\n",
    "    def forward(self,query,key,value,mask=None):  # q,k,v: [batch, seq_len, embedding_dim]\n",
    "        nbatches = query.shape[0]  #批数量\n",
    "        query, key, value = [\n",
    "            l(x) for l, x in zip(self.linears,\n",
    "                            (query.to(torch.float32),\n",
    "                             key.to(torch.float32),\n",
    "                             value.to(torch.float32)))\n",
    "        ]  #获取zip的query,key,value权重矩阵\n",
    "        attn, p_atten = attention(query,key,value,mask=mask,dropout=self.dropout)\n",
    "        out = self.linears[-1](attn)  #得到最后一层线性层的输出\n",
    "        return out,p_atten  #返回out结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "829bd2f8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-03T08:26:34.843912Z",
     "start_time": "2023-06-03T08:26:34.803944Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "from utils import metricsCal\n",
    "from torch.utils.data import DataLoader,TensorDataset\n",
    "import copy\n",
    "from torch.autograd import Variable\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "def train(model,data,label,epoch,train_device,model_dir,batch_size):\n",
    "    if os.path.exists(model_dir+'model.pt'):\n",
    "        model_train = torch.load(model_dir+'/model.pt')\n",
    "    else:\n",
    "        model_train = model\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = torch.optim.Adam(model_train.parameters(),lr=0.003)  #改变学习率\n",
    "    dataX = torch.Tensor(data).clone().detach()\n",
    "    label = torch.Tensor(label).clone().detach()\n",
    "    train_data = TensorDataset(dataX, label)\n",
    "    train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
    "    running_loss = 0.0\n",
    "    model_train = model_train.to(train_device)\n",
    "    for batch_idx,data in enumerate(train_loader,0):\n",
    "        inputs,target = data\n",
    "        #inputs = inputs.reshape(inputs.shape[0], 1, inputs.shape[1])\n",
    "        inputs = inputs.to(train_device)\n",
    "        target = target.to(train_device)\n",
    "        target = target.reshape(target.shape[0],1)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model_train(inputs)\n",
    "        loss = criterion(outputs,target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        #print(type(loss))\n",
    "        running_loss += loss.item()\n",
    "        if batch_idx == len(dataX)//batch_size:\n",
    "            #print('[%d, %5d] epoch loss: %.3f' %(epoch+1,batch_idx+1,running_loss))\n",
    "            print(running_loss)\n",
    "    save_model(model_train,model_dir)\n",
    "    model_train = torch.load(model_dir+'/model.pt')\n",
    "    #device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")  # 选择设备\n",
    "    th,_,_,_,_,_,_,_,_,_ = metricsCal.evaluate(model_train,train_loader,train_device)\n",
    "    return running_loss,th\n",
    "\n",
    "def test(data,label,best_auc,test_device,model_dir,batch_size,th):\n",
    "    model_test = load_model(model_dir)\n",
    "    #model_test.eval()\n",
    "    #model_test.to(test_device)\n",
    "    data = torch.Tensor(data).clone().detach()#torch.Tensor(data)\n",
    "    label = torch.Tensor(label).clone().detach()#.requires_grad_(True)torch.Tensor(label)\n",
    "    test_data = TensorDataset(data,label)\n",
    "    test_loader = DataLoader(test_data, shuffle=False, batch_size=batch_size)\n",
    "    #device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")  # 选择设备\n",
    "    _,_,_,_,Sen,Spe, Acc, mcc, AUC = metricsCal.evaluate(model_test,test_loader,test_device,False,th)\n",
    "    \n",
    "    print('Accuracy on test set: %d %%' %Acc)\n",
    "    print('Sensitivity on test set: %d %%' %Sen)\n",
    "    print('Speciality on test set: %d %%' %Spe)\n",
    "    print('MCC on test set: %.3f' %mcc)\n",
    "    print('auc on test set: %.3f' %AUC)\n",
    "    if(AUC > best_auc):\n",
    "        torch.save(model_test,model_dir+'model_best.pt')\n",
    "    return Sen,Spe,Acc, mcc, AUC\n",
    "\n",
    "def independTest(data,label,test_device,model_dir,batch_size,th):\n",
    "    model_test = load_bestModel(model_dir)\n",
    "    data = torch.Tensor(data).clone().detach()#torch.Tensor(data)\n",
    "    label = torch.Tensor(label).clone().detach()#.requires_grad_(True)torch.Tensor(label)\n",
    "    test_data = TensorDataset(data,label)\n",
    "    test_loader = DataLoader(test_data, shuffle=False, batch_size=batch_size)\n",
    "    #device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")  # 选择设备\n",
    "    _,_,_,_,Sen,Spe, Acc, mcc, AUC = metricsCal.evaluate(model_test,test_loader,test_device,False,th)\n",
    "    print(Acc,mcc,AUC)\n",
    "    print('Accuracy on test set: %d %%' %Acc)\n",
    "    print('Sensitivity on test set: %d %%' %Sen)\n",
    "    print('Speciality on test set: %d %%' %Spe)\n",
    "    print('MCC on test set: %.3f' %mcc)\n",
    "    print('auc on test set: %.3f' %AUC)\n",
    "    return Acc, mcc, AUC\n",
    "\n",
    "def load_model(model_dir):\n",
    "    if os.path.exists(model_dir+'model.pt'):\n",
    "        model_load = torch.load(model_dir+'model.pt')\n",
    "    return model_load\n",
    "\n",
    "def save_model(model_save,model_dir):\n",
    "    if not os.path.exists(model_dir):\n",
    "        os.makedirs(model_dir)\n",
    "    torch.save(model_save, model_dir+'model.pt')\n",
    "\n",
    "def load_bestModel(model_dir):\n",
    "    if os.path.exists(model_dir+'model_best.pt'):\n",
    "        model_load = torch.load(model_dir+'model_best.pt')#,map_location='cuda:0')\n",
    "    return model_load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cd00e8a7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-03T08:35:53.753932Z",
     "start_time": "2023-06-03T08:35:53.657268Z"
    }
   },
   "outputs": [],
   "source": [
    "#CNN\n",
    "class Model_CNN(nn.Module):\n",
    "    def __init__(self, dim1, dim2, dropout=0.1):\n",
    "        super(Model_CNN, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(1,3,kernel_size=5,stride=2,padding=2)\n",
    "        self.conv2 = nn.Conv2d(3,3,kernel_size=5,stride=2,padding=2)\n",
    "        #self.conv3 = nn.Conv2d(3,3,kernel_size=5,stride=2,padding=2)\n",
    "        \n",
    "        k1 = int((dim1+2*2-5)/2)+1\n",
    "        k2 = int((dim2+2*2-5)/2)+1\n",
    "        k1 = int((k1+2*2-5)/2)+1\n",
    "        k2 = int((k2+2*2-5)/2)+1\n",
    "        #k1 = int((k1+2*2-5)/2)+1\n",
    "        #k2 = int((k2+2*2-5)/2)+1\n",
    "        \n",
    "        self.fn1 = nn.Linear(3*k1*k2,128)\n",
    "        self.fn2 = nn.Linear(128,1)\n",
    "        self.ac = nn.Sigmoid()\n",
    "        self.bm1 = nn.BatchNorm1d(dim1)\n",
    "        \n",
    "        self.relu = nn.ReLU()#inplace=True)\n",
    "        self.drop = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x1 = x.contiguous().view(-1,1,x.shape[1],x.shape[2])\n",
    "        x1 = self.conv1(x1)\n",
    "        x1 = self.relu(x1)\n",
    "        x1 = self.conv2(x1)\n",
    "        x1 = self.relu(x1)\n",
    "        #x1 = self.conv3(x1)\n",
    "        #x1 = self.relu(x1)\n",
    "        x1 = x1.contiguous().view(x1.shape[0],-1)\n",
    "        x1 = self.fn1(x1)\n",
    "        x1 = self.relu(x1)\n",
    "        x1 = self.drop(x1)\n",
    "        x1 = self.fn2(x1)\n",
    "        out = self.ac(x1)\n",
    "        return out\n",
    "\n",
    "#LSTM\n",
    "class Model_LSTM(nn.Module):\n",
    "    def __init__(self, dim1, dim2, dropout=0.1):\n",
    "        super(Model_LSTM, self).__init__()\n",
    "        self.posi = PositionalEncoding(dim1,dim2,dropout)\n",
    "        self.lstm = nn.LSTM(input_size=dim2,hidden_size=dim2,batch_first=True,bidirectional=False)\n",
    "        self.bm1 = nn.BatchNorm1d(dim1)\n",
    "        self.conv1 = nn.Conv2d(1,1,kernel_size=5,stride=2,padding=2)\n",
    "        k1 = int((dim1+2*2-5)/2)+1\n",
    "        k2 = int((dim2+2*2-5)/2)+1\n",
    "        self.fn1 = nn.Linear(1*k1*k2,128)\n",
    "        #self.fn1 = nn.Linear(dim1*dim2,128)\n",
    "        self.fn2 = nn.Linear(128,1)\n",
    "        self.ac = nn.Sigmoid()\n",
    "        \n",
    "        self.relu = nn.ReLU()#inplace=True)\n",
    "        self.drop = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #所有输入，如果只有一维，需要变成1*n维\n",
    "        x1 = self.posi(x)\n",
    "        x1,(h_n,c_n) = self.lstm(x1)\n",
    "        x1 = self.relu(x1)\n",
    "        x1 = self.bm1(x1)\n",
    "        \n",
    "        x1 = x1.contiguous().view(-1,1,x1.shape[1],x1.shape[2])\n",
    "        x1 = self.conv1(x1)\n",
    "        x1 = self.relu(x1)\n",
    "        x1 = x1.contiguous().view(x1.shape[0],-1)\n",
    "        x1 = self.fn1(x1)\n",
    "        x1 = self.relu(x1)\n",
    "        x1 = self.drop(x1)\n",
    "        x1 = self.fn2(x1)\n",
    "        out = self.ac(x1)\n",
    "        return out\n",
    "\n",
    "#BiLSTM\n",
    "class Model_BiLSTM(nn.Module):\n",
    "    def __init__(self, dim1, dim2, dropout=0.1):\n",
    "        super(Model_BiLSTM, self).__init__()\n",
    "        self.posi = PositionalEncoding(dim1,dim2,dropout)\n",
    "        self.lstm = nn.LSTM(input_size=dim2,hidden_size=dim2,batch_first=True,bidirectional=True)\n",
    "        self.bm1 = nn.BatchNorm1d(dim1)\n",
    "        self.conv1 = nn.Conv2d(1,1,kernel_size=5,stride=2,padding=2)\n",
    "        k1 = int((dim1+2*2-5)/2)+1\n",
    "        k2 = int((dim2+2*2-5)/2)+1\n",
    "        self.fn1 = nn.Linear(1*k1*k2,128)\n",
    "        self.fn2 = nn.Linear(128,1)\n",
    "        self.ac = nn.Sigmoid()\n",
    "        \n",
    "        self.relu = nn.ReLU()#inplace=True)\n",
    "        self.drop = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #所有输入，如果只有一维，需要变成1*n维\n",
    "        x1 = self.posi(x)\n",
    "        x1,(h_n,c_n) = self.lstm(x1)\n",
    "        x1 = x1[:,:,0:339]+x1[:,:,339:678]\n",
    "        x1 = self.relu(x1)\n",
    "        x1 = self.bm1(x1)\n",
    "        x1 = x1.contiguous().view(-1,1,x1.shape[1],x1.shape[2])\n",
    "        x1 = self.conv1(x1)\n",
    "        x1 = self.relu(x1)\n",
    "        x1 = x1.contiguous().view(x1.shape[0],-1)\n",
    "        x1 = self.fn1(x1)\n",
    "        x1 = self.relu(x1)\n",
    "        x1 = self.drop(x1)\n",
    "        x1 = self.fn2(x1)\n",
    "        out = self.ac(x1)\n",
    "        return out\n",
    "    \n",
    "#Attention\n",
    "class Model_Attention(nn.Module):\n",
    "    def __init__(self, dim1, dim2, dropout=0.1):\n",
    "        super(Model_Attention, self).__init__()\n",
    "        self.posi = PositionalEncoding(dim1,dim2,dropout)\n",
    "        self.self_A = SelfAttention(dim2)\n",
    "        self.self_B = SelfAttention(dim1)\n",
    "        self.bm1 = nn.BatchNorm1d(dim1)\n",
    "        self.conv1 = nn.Conv2d(1,1,kernel_size=5,stride=2,padding=2)\n",
    "        k1 = int((dim1+2*2-5)/2)+1\n",
    "        k2 = int((dim2+2*2-5)/2)+1\n",
    "        self.fn1 = nn.Linear(1*k1*k2,128)\n",
    "        self.fn2 = nn.Linear(128,1)\n",
    "        self.ac = nn.Sigmoid()\n",
    "        self.bm1 = nn.BatchNorm1d(dim1)\n",
    "        \n",
    "        self.relu = nn.ReLU()#inplace=True)\n",
    "        self.drop = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #所有输入，如果只有一维，需要变成1*n维\n",
    "        x1 = self.posi(x)\n",
    "        if x1.shape[1] == 1:\n",
    "            x1 = x1.view(x1.shape[0],x1.shape[2],x1.shape[1])\n",
    "            x1,attn = self.self_B(x1,x1,x1)\n",
    "        else:\n",
    "            x1,attn = self.self_A(x1,x1,x1)\n",
    "        if x1.shape[2]==1:\n",
    "            x1 = x1.view(x1.shape[0],x1.shape[2],x1.shape[1])\n",
    "        x1 = self.bm1(x1)\n",
    "        x1 = self.relu(x1)\n",
    "        \n",
    "        x1 = x1.contiguous().view(-1,1,x1.shape[1],x1.shape[2])\n",
    "        x1 = self.conv1(x1)\n",
    "        x1 = self.relu(x1)\n",
    "        x1 = x1.contiguous().view(x1.shape[0],-1)\n",
    "        x1 = self.fn1(x1)\n",
    "        x1 = self.relu(x1)\n",
    "        x1 = self.drop(x1)\n",
    "        x1 = self.fn2(x1)\n",
    "        out = self.ac(x1)\n",
    "        return out\n",
    "\n",
    "#LSTM_Attention_CNN\n",
    "class Model_LSTM_Attention_CNN(nn.Module):\n",
    "    def __init__(self, dim1, dim2, dropout=0.1):\n",
    "        super(Model_LSTM_Attention_CNN, self).__init__()\n",
    "        self.posi = PositionalEncoding(dim1,dim2,dropout)\n",
    "        self.self_A = SelfAttention(dim2)\n",
    "        self.self_B = SelfAttention(dim1)\n",
    "        self.lstm = nn.LSTM(input_size=dim2,hidden_size=dim2,batch_first=True,bidirectional=False)\n",
    "        self.bm1 = nn.BatchNorm1d(dim1)\n",
    "        self.conv1 = nn.Conv2d(1,1,kernel_size=5,stride=2,padding=2)\n",
    "        k1 = int((dim1+2*2-5)/2)+1\n",
    "        k2 = int((dim2+2*2-5)/2)+1\n",
    "        self.fn1 = nn.Linear(1*k1*k2,128)\n",
    "        self.fn2 = nn.Linear(128,1)\n",
    "        self.ac = nn.Sigmoid()\n",
    "        self.bm1 = nn.BatchNorm1d(dim1)\n",
    "        \n",
    "        self.relu = nn.ReLU()#inplace=True)\n",
    "        self.drop = nn.Dropout(p=dropout)\n",
    "        #self.encoder = nn.Sequential()\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        #所有输入，如果只有一维，需要变成1*n维\n",
    "        x1 = self.posi(x)\n",
    "        x1,(h_n,c_n) = self.lstm(x1)\n",
    "        x1 = self.relu(x1)\n",
    "        if x1.shape[1] == 1:\n",
    "            x1 = x1.view(x1.shape[0],x1.shape[2],x1.shape[1])\n",
    "            x1,attn = self.self_B(x1,x1,x1)\n",
    "        else:\n",
    "            x1,attn = self.self_A(x1,x1,x1)\n",
    "        if x1.shape[2]==1:\n",
    "            x1 = x1.view(x1.shape[0],x1.shape[2],x1.shape[1])\n",
    "        x1 = self.bm1(x1)\n",
    "        x1 = self.relu(x1)\n",
    "        \n",
    "        x1 = x1.contiguous().view(-1,1,x1.shape[1],x1.shape[2])\n",
    "        x1 = self.conv1(x1)\n",
    "        x1 = self.relu(x1)\n",
    "        x1 = x1.contiguous().view(x1.shape[0],-1)\n",
    "        x1 = self.fn1(x1)\n",
    "        x1 = self.relu(x1)\n",
    "        x1 = self.drop(x1)\n",
    "        x1 = self.fn2(x1)\n",
    "        out = self.ac(x1)\n",
    "        return out\n",
    "    \n",
    "#BiLSTM_Attention_CNN\n",
    "class Model_BiLSTM_Attention_CNN(nn.Module):\n",
    "    def __init__(self, dim1, dim2, dropout=0.1):\n",
    "        super(Model_BiLSTM_Attention_CNN, self).__init__()\n",
    "        self.posi = PositionalEncoding(dim1,dim2,dropout)\n",
    "        self.self_A = SelfAttention(dim2)\n",
    "        self.self_B = SelfAttention(dim1)\n",
    "        self.lstm = nn.LSTM(input_size=dim2,hidden_size=dim2,batch_first=True,bidirectional=True)\n",
    "        self.bm1 = nn.BatchNorm1d(dim1)\n",
    "        self.conv1 = nn.Conv2d(1,1,kernel_size=5,stride=2,padding=2)\n",
    "        k1 = int((dim1+2*2-5)/2)+1\n",
    "        k2 = int((dim2+2*2-5)/2)+1\n",
    "        self.fn1 = nn.Linear(1*k1*k2,128)\n",
    "        self.fn2 = nn.Linear(128,1)\n",
    "        self.ac = nn.Sigmoid()\n",
    "        self.bm1 = nn.BatchNorm1d(dim1)\n",
    "        \n",
    "        self.relu = nn.ReLU()#inplace=True)\n",
    "        self.drop = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        #所有输入，如果只有一维，需要变成1*n维\n",
    "        x1 = self.posi(x)\n",
    "        x1,(h_n,c_n) = self.lstm(x1)\n",
    "        x1 = x1[:,:,0:339]+x1[:,:,339:678]\n",
    "        x1 = self.relu(x1)\n",
    "        if x1.shape[1] == 1:\n",
    "            x1 = x1.view(x1.shape[0],x1.shape[2],x1.shape[1])\n",
    "            x1,attn = self.self_B(x1,x1,x1)\n",
    "        else:\n",
    "            x1,attn = self.self_A(x1,x1,x1)\n",
    "        if x1.shape[2]==1:\n",
    "            x1 = x1.view(x1.shape[0],x1.shape[2],x1.shape[1])\n",
    "        x1 = self.bm1(x1)\n",
    "        x1 = self.relu(x1)\n",
    "        \n",
    "        x1 = x1.contiguous().view(-1,1,x1.shape[1],x1.shape[2])\n",
    "        x1 = self.conv1(x1)\n",
    "        x1 = self.relu(x1)\n",
    "        x1 = x1.contiguous().view(x1.shape[0],-1)\n",
    "        x1 = self.fn1(x1)\n",
    "        x1 = self.relu(x1)\n",
    "        x1 = self.drop(x1)\n",
    "        x1 = self.fn2(x1)\n",
    "        out = self.ac(x1)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "547186ca",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-03T08:27:03.269029Z",
     "start_time": "2023-06-03T08:27:03.260511Z"
    }
   },
   "outputs": [],
   "source": [
    "def independResult(data,label,test_device,model_dir,batch_size,th=0.5):\n",
    "    model_test = load_bestModel(model_dir)\n",
    "    data = torch.Tensor(data).clone().detach()#torch.Tensor(data)\n",
    "    label = torch.Tensor(label).clone().detach()#.requires_grad_(True)torch.Tensor(label)\n",
    "    test_data = TensorDataset(data,label)\n",
    "    test_loader = DataLoader(test_data, shuffle=False, batch_size=batch_size)\n",
    "    #device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")  # 选择设备\n",
    "    \n",
    "    return evaluate_result(model_test,test_loader,test_device,False)\n",
    "\n",
    "def evaluate_result(model, dataloader, device, is_train=True, threshold=0.5):\n",
    "    model.eval()\n",
    "    y_true = torch.tensor([],dtype=torch.int)\n",
    "    y_score = torch.tensor([])\n",
    "    #for data in tqdm(dataloader):\n",
    "    model = model.to(device)\n",
    "    for data in dataloader:\n",
    "        #if not isinstance(model, Res_Net):\n",
    "        if 1==1:\n",
    "            inputs,y = data\n",
    "            inputs = inputs.to(device)\n",
    "            out = model(inputs)\n",
    "        out = out.squeeze(dim=-1)\n",
    "        #out = torch.sigmoid(out)\n",
    "        y_true = torch.cat((y_true, y.int().detach().cpu()))\n",
    "        y_score = torch.cat((y_score, out.detach().cpu()))  #detach去除梯度，然后cpu()，然后cat将其连接起来\n",
    "    y_true = y_true.numpy()\n",
    "    y_score = y_score.numpy()\n",
    "    return y_score, y_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1cd576e3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-03T08:45:39.570787Z",
     "start_time": "2023-06-03T08:45:39.566075Z"
    }
   },
   "outputs": [],
   "source": [
    "X_All = []\n",
    "Label = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1bf23197",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-03T08:27:26.896944Z",
     "start_time": "2023-06-03T08:27:26.889162Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9126, 1, 339)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainData.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ba703bab",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-03T08:37:11.593236Z",
     "start_time": "2023-06-03T08:37:08.169643Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/anaconda3/envs/BERT/lib/python3.6/site-packages/sklearn/utils/validation.py:72: FutureWarning: Pass shuffle=True, random_state=0 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error\n",
      "  \"will result in an error\", FutureWarning)\n",
      "/data/xiayunpeng/anaconda3/envs/BERT/lib/python3.6/site-packages/ipykernel_launcher.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/data/xiayunpeng/anaconda3/envs/BERT/lib/python3.6/site-packages/ipykernel_launcher.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/data/xiayunpeng/anaconda3/envs/BERT/lib/python3.6/site-packages/ipykernel_launcher.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/data/xiayunpeng/anaconda3/envs/BERT/lib/python3.6/site-packages/ipykernel_launcher.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/data/xiayunpeng/anaconda3/envs/BERT/lib/python3.6/site-packages/ipykernel_launcher.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/data/xiayunpeng/anaconda3/envs/BERT/lib/python3.6/site-packages/ipykernel_launcher.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/data/xiayunpeng/anaconda3/envs/BERT/lib/python3.6/site-packages/ipykernel_launcher.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/data/xiayunpeng/anaconda3/envs/BERT/lib/python3.6/site-packages/ipykernel_launcher.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/data/xiayunpeng/anaconda3/envs/BERT/lib/python3.6/site-packages/ipykernel_launcher.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/data/xiayunpeng/anaconda3/envs/BERT/lib/python3.6/site-packages/ipykernel_launcher.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "from sklearn.metrics import roc_auc_score\n",
    "#先做交叉验证，看看多少个epoch合适\n",
    "rows = 10\n",
    "row = -1\n",
    "max_epochs = 3\n",
    "max_patience = 25\n",
    "batch_size = 256\n",
    "\n",
    "X_BiLSTM_Attention_CNN = []\n",
    "test_BiLSTM_Attention_CNN = []\n",
    "\n",
    "#然后全部all_in 一个模型出来\n",
    "#%env CUDA_LAUNCH_BLOCKING=1\n",
    "kf = KFold(10,True,0)\n",
    "x = 0\n",
    "#第三重循环\n",
    "for i,[train_index, test_index] in enumerate(kf.split(trainData)):\n",
    "\n",
    "    X_train = trainData[train_index]\n",
    "    X_test = trainData[test_index]\n",
    "    Y_train = trainLabel[train_index]\n",
    "    Y_test = trainLabel[test_index]\n",
    "    \n",
    "    model_dir = \"Model/Model_BiLSTM_Attention_CNN_loc/KFold_\" + str(i)+\"/\"\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")  #选择设备\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "\n",
    "    y_pred,y_true = independResult(X_test,Y_test,device,model_dir,batch_size)\n",
    "    X_BiLSTM_Attention_CNN.append(y_pred)\n",
    "    \n",
    "    test_pred,_ = independResult(testData,testLabel,device,model_dir,batch_size)\n",
    "    test_BiLSTM_Attention_CNN.append(test_pred)\n",
    "    #Label.append(Y_test)\n",
    "    x += roc_auc_score(testLabel,test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c39dcf3d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-03T08:37:13.543678Z",
     "start_time": "2023-06-03T08:37:13.538107Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.7462170000000015"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d3c3aa8f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-03T08:40:36.081593Z",
     "start_time": "2023-06-03T08:40:11.398665Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/anaconda3/envs/BERT/lib/python3.6/site-packages/sklearn/decomposition/_fastica.py:120: ConvergenceWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.\n",
      "  ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import FastICA\n",
    "ica = FastICA(n_components= 100).fit(trainData_loc)\n",
    "trainData = ica.transform(trainData_loc)\n",
    "testData = ica.transform(testData_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9d1114dd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-03T08:41:50.643735Z",
     "start_time": "2023-06-03T08:41:50.606120Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9126, 100)\n"
     ]
    }
   ],
   "source": [
    "list_fea = list(pd.read_csv(\"Result/Feature_Selection/Location_ICA_boruta_shap.csv\").iloc[0:100,1])\n",
    "feature = []\n",
    "for i in range(len(list_fea)):\n",
    "    feature.append(trainData[:,int(list_fea[i][4:])])\n",
    "trainData = np.array(feature).T\n",
    "\n",
    "feature = []\n",
    "for i in range(len(list_fea)):\n",
    "    feature.append(testData[:,int(list_fea[i][4:])])\n",
    "testData = np.array(feature).T\n",
    "\n",
    "print(trainData.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "46bbc1fa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-03T08:45:55.310538Z",
     "start_time": "2023-06-03T08:45:51.189474Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiayunpeng/anaconda3/envs/BERT/lib/python3.6/site-packages/sklearn/utils/validation.py:72: FutureWarning: Pass shuffle=True, random_state=0 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error\n",
      "  \"will result in an error\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "kf = KFold(10,True,0)\n",
    "from thundersvm import SVC\n",
    "X_SVM = []\n",
    "test_SVM = []\n",
    "for i,[train_index, test_index] in enumerate(kf.split(trainData)):\n",
    "\n",
    "    X_train = trainData[train_index]\n",
    "    X_test = trainData[test_index]\n",
    "    Y_train = trainLabel[train_index]\n",
    "    Y_test = trainLabel[test_index]\n",
    "    \n",
    "    clf_svm = SVC(C=21.5237429100287, gamma=5.536621106133444)\n",
    "    \n",
    "    clf_svm.fit(X_train,Y_train)\n",
    "\n",
    "    X_SVM.append(   np.column_stack(#(clf_rf.predict_proba(X_test)[:,1],\n",
    "               # clf_et.predict_proba(X_test)[:,1]))\n",
    "               # clf_lgbm.predict_proba(X_test)[:,1],\n",
    "               # clf_et.predict_proba(X_test)[:,1],\n",
    "               # clf_gb.predict_proba(X_test)[:,1],\n",
    "               clf_svm.decision_function(X_test)))\n",
    "    \n",
    "    test_SVM.append(   np.column_stack(#(clf_rf.predict_proba(X_test)[:,1],\n",
    "               # clf_et.predict_proba(X_test)[:,1]))\n",
    "               # clf_lgbm.predict_proba(X_test)[:,1],\n",
    "               # clf_et.predict_proba(X_test)[:,1],\n",
    "               # clf_gb.predict_proba(X_test)[:,1],\n",
    "               clf_svm.decision_function(testData)))\n",
    "    \n",
    "    Label.append(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e1033b8f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-03T08:45:59.083950Z",
     "start_time": "2023-06-03T08:45:59.060314Z"
    }
   },
   "outputs": [],
   "source": [
    "Y_kk = np.hstack((Label[0],Label[1],Label[2],Label[3],Label[4],Label[5],Label[6],Label[7],Label[8],Label[9]))\n",
    "\n",
    "valData_CNN = np.hstack((X_CNN[0],X_CNN[1],X_CNN[2],X_CNN[3],X_CNN[4],X_CNN[5],X_CNN[6],X_CNN[7],X_CNN[8],X_CNN[9]))\n",
    "test_CNN = np.vstack((test_CNN[0],test_CNN[1],test_CNN[2],test_CNN[3],test_CNN[4],test_CNN[5],test_CNN[6],test_CNN[7],test_CNN[8],test_CNN[9]))\n",
    "\n",
    "valData_LSTM = np.hstack((X_LSTM[0],X_LSTM[1],X_LSTM[2],X_LSTM[3],X_LSTM[4],X_LSTM[5],X_LSTM[6],X_LSTM[7],X_LSTM[8],X_LSTM[9]))\n",
    "test_LSTM = np.vstack((test_LSTM[0],test_LSTM[1],test_LSTM[2],test_LSTM[3],test_LSTM[4],test_LSTM[5],test_LSTM[6],test_LSTM[7],test_LSTM[8],test_LSTM[9]))\n",
    "\n",
    "valData_BiLSTM = np.hstack((X_BiLSTM[0],X_BiLSTM[1],X_BiLSTM[2],X_BiLSTM[3],X_BiLSTM[4],X_BiLSTM[5],X_BiLSTM[6],X_BiLSTM[7],X_BiLSTM[8],X_BiLSTM[9]))\n",
    "test_BiLSTM = np.vstack((test_BiLSTM[0],test_BiLSTM[1],test_BiLSTM[2],test_BiLSTM[3],test_BiLSTM[4],test_BiLSTM[5],test_BiLSTM[6],test_BiLSTM[7],test_BiLSTM[8],test_BiLSTM[9]))\n",
    "\n",
    "valData_LSTM_Attention_CNN = np.hstack((X_LSTM_Attention_CNN[0],X_LSTM_Attention_CNN[1],X_LSTM_Attention_CNN[2],X_LSTM_Attention_CNN[3],\n",
    "                                        X_LSTM_Attention_CNN[4],X_LSTM_Attention_CNN[5],X_LSTM_Attention_CNN[6],X_LSTM_Attention_CNN[7],\n",
    "                                        X_LSTM_Attention_CNN[8],X_LSTM_Attention_CNN[9]))\n",
    "test_LSTM_Attention_CNN = np.vstack((test_LSTM_Attention_CNN[0],test_LSTM_Attention_CNN[1],test_LSTM_Attention_CNN[2],test_LSTM_Attention_CNN[3],\n",
    "                                     test_LSTM_Attention_CNN[4],test_LSTM_Attention_CNN[5],test_LSTM_Attention_CNN[6],test_LSTM_Attention_CNN[7],\n",
    "                                     test_LSTM_Attention_CNN[8],test_LSTM_Attention_CNN[9]))\n",
    "\n",
    "valData_BiLSTM_Attention_CNN = np.hstack((X_BiLSTM_Attention_CNN[0],X_BiLSTM_Attention_CNN[1],X_BiLSTM_Attention_CNN[2],X_BiLSTM_Attention_CNN[3],X_BiLSTM_Attention_CNN[4],X_BiLSTM_Attention_CNN[5],X_BiLSTM_Attention_CNN[6],X_BiLSTM_Attention_CNN[7],X_BiLSTM_Attention_CNN[8],X_BiLSTM_Attention_CNN[9]))\n",
    "test_BiLSTM_Attention_CNN = np.vstack((test_BiLSTM_Attention_CNN[0],test_BiLSTM_Attention_CNN[1],test_BiLSTM_Attention_CNN[2],test_BiLSTM_Attention_CNN[3],test_BiLSTM_Attention_CNN[4],test_BiLSTM_Attention_CNN[5],test_BiLSTM_Attention_CNN[6],test_BiLSTM_Attention_CNN[7],test_BiLSTM_Attention_CNN[8],test_BiLSTM_Attention_CNN[9]))\n",
    "\n",
    "valData_SVM = np.hstack((X_SVM[0],X_SVM[1],X_SVM[2],X_SVM[3],X_SVM[4],X_SVM[5],X_SVM[6],X_SVM[7],X_SVM[8],X_SVM[9]))\n",
    "test_SVM = np.vstack((test_SVM[0],test_SVM[1],test_SVM[2],test_SVM[3],test_SVM[4],test_SVM[5],test_SVM[6],test_SVM[7],test_SVM[8],test_SVM[9]))\n",
    "\n",
    "#pd.DataFrame(np.append(valData_kk.reshape(-1,1),Y_kk.reshape(-1,1),axis=1)).to_csv(\"\",header=[\"pred\",\"true\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d1021690",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-03T08:46:08.373162Z",
     "start_time": "2023-06-03T08:46:08.214975Z"
    }
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(np.vstack((valData_CNN,valData_LSTM,valData_BiLSTM,valData_LSTM_Attention_CNN,valData_BiLSTM_Attention_CNN,valData_SVM,Y_kk)).T).to_csv(\"Result/loc_result_val.csv\",header=['CNN','LSTM','BiLSTM','LSTM_Attention_CNN','BiLSTM_Attention_CNN','SVM','True'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1fede8a5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-03T08:46:58.281338Z",
     "start_time": "2023-06-03T08:46:58.275176Z"
    }
   },
   "outputs": [],
   "source": [
    "kl = ['CNN', 'LSTM', 'BiLSTM', 'LSTM_Attention_CNN', 'BiLSTM_Attention_CNN', 'SVM']\n",
    "ml = []\n",
    "for i in range(len(kl)):\n",
    "    for j in range(10):\n",
    "        ml.append(str(kl[i])+\"_KFold_\"+str(j) )\n",
    "ml.append(\"True\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9592beab",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-03T08:47:26.858876Z",
     "start_time": "2023-06-03T08:47:26.590996Z"
    }
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(np.vstack((test_CNN,test_LSTM,test_BiLSTM,test_LSTM_Attention_CNN,test_BiLSTM_Attention_CNN,test_SVM,testLabel)).T).to_csv(\"Result/loc_result_test.csv\",header=ml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ca95c75f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-03T08:47:38.423115Z",
     "start_time": "2023-06-03T08:47:38.349262Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>CNN_KFold_0</th>\n",
       "      <th>CNN_KFold_1</th>\n",
       "      <th>CNN_KFold_2</th>\n",
       "      <th>CNN_KFold_3</th>\n",
       "      <th>CNN_KFold_4</th>\n",
       "      <th>CNN_KFold_5</th>\n",
       "      <th>CNN_KFold_6</th>\n",
       "      <th>CNN_KFold_7</th>\n",
       "      <th>CNN_KFold_8</th>\n",
       "      <th>...</th>\n",
       "      <th>SVM_KFold_1</th>\n",
       "      <th>SVM_KFold_2</th>\n",
       "      <th>SVM_KFold_3</th>\n",
       "      <th>SVM_KFold_4</th>\n",
       "      <th>SVM_KFold_5</th>\n",
       "      <th>SVM_KFold_6</th>\n",
       "      <th>SVM_KFold_7</th>\n",
       "      <th>SVM_KFold_8</th>\n",
       "      <th>SVM_KFold_9</th>\n",
       "      <th>True</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.486822</td>\n",
       "      <td>0.522864</td>\n",
       "      <td>0.619165</td>\n",
       "      <td>0.621922</td>\n",
       "      <td>0.671618</td>\n",
       "      <td>0.606191</td>\n",
       "      <td>0.576225</td>\n",
       "      <td>0.600986</td>\n",
       "      <td>0.482496</td>\n",
       "      <td>...</td>\n",
       "      <td>0.401630</td>\n",
       "      <td>0.583595</td>\n",
       "      <td>0.576984</td>\n",
       "      <td>0.519101</td>\n",
       "      <td>0.511225</td>\n",
       "      <td>0.492760</td>\n",
       "      <td>0.493398</td>\n",
       "      <td>0.478668</td>\n",
       "      <td>0.458823</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.919098</td>\n",
       "      <td>0.916081</td>\n",
       "      <td>0.904019</td>\n",
       "      <td>0.927956</td>\n",
       "      <td>0.931615</td>\n",
       "      <td>0.932323</td>\n",
       "      <td>0.883166</td>\n",
       "      <td>0.924349</td>\n",
       "      <td>0.827342</td>\n",
       "      <td>...</td>\n",
       "      <td>2.217742</td>\n",
       "      <td>1.874554</td>\n",
       "      <td>2.009260</td>\n",
       "      <td>1.979583</td>\n",
       "      <td>1.912362</td>\n",
       "      <td>2.076056</td>\n",
       "      <td>1.920187</td>\n",
       "      <td>1.975787</td>\n",
       "      <td>2.030250</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.199294</td>\n",
       "      <td>0.228452</td>\n",
       "      <td>0.256258</td>\n",
       "      <td>0.443887</td>\n",
       "      <td>0.354461</td>\n",
       "      <td>0.215887</td>\n",
       "      <td>0.270216</td>\n",
       "      <td>0.283867</td>\n",
       "      <td>0.208107</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.980761</td>\n",
       "      <td>-1.095997</td>\n",
       "      <td>-1.237363</td>\n",
       "      <td>-0.961593</td>\n",
       "      <td>-1.126832</td>\n",
       "      <td>-1.074842</td>\n",
       "      <td>-1.013983</td>\n",
       "      <td>-1.019705</td>\n",
       "      <td>-0.916501</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.540013</td>\n",
       "      <td>0.486236</td>\n",
       "      <td>0.487673</td>\n",
       "      <td>0.521803</td>\n",
       "      <td>0.563720</td>\n",
       "      <td>0.636915</td>\n",
       "      <td>0.476417</td>\n",
       "      <td>0.665422</td>\n",
       "      <td>0.398513</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.104977</td>\n",
       "      <td>-0.164274</td>\n",
       "      <td>-0.162744</td>\n",
       "      <td>-0.116986</td>\n",
       "      <td>-0.173945</td>\n",
       "      <td>0.010671</td>\n",
       "      <td>-0.194698</td>\n",
       "      <td>-0.060514</td>\n",
       "      <td>-0.113337</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.836471</td>\n",
       "      <td>0.806130</td>\n",
       "      <td>0.798109</td>\n",
       "      <td>0.827323</td>\n",
       "      <td>0.856633</td>\n",
       "      <td>0.864365</td>\n",
       "      <td>0.767971</td>\n",
       "      <td>0.864445</td>\n",
       "      <td>0.682569</td>\n",
       "      <td>...</td>\n",
       "      <td>0.958914</td>\n",
       "      <td>0.873867</td>\n",
       "      <td>0.853417</td>\n",
       "      <td>0.859337</td>\n",
       "      <td>0.902361</td>\n",
       "      <td>1.035411</td>\n",
       "      <td>1.018865</td>\n",
       "      <td>1.006281</td>\n",
       "      <td>0.921924</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>1995</td>\n",
       "      <td>0.142740</td>\n",
       "      <td>0.145378</td>\n",
       "      <td>0.300693</td>\n",
       "      <td>0.224134</td>\n",
       "      <td>0.347749</td>\n",
       "      <td>0.163878</td>\n",
       "      <td>0.292498</td>\n",
       "      <td>0.160482</td>\n",
       "      <td>0.213052</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.096920</td>\n",
       "      <td>-1.131857</td>\n",
       "      <td>-1.115432</td>\n",
       "      <td>-0.862313</td>\n",
       "      <td>-1.039431</td>\n",
       "      <td>-1.043558</td>\n",
       "      <td>-1.002878</td>\n",
       "      <td>-0.905958</td>\n",
       "      <td>-1.083364</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>1996</td>\n",
       "      <td>0.849183</td>\n",
       "      <td>0.891769</td>\n",
       "      <td>0.900345</td>\n",
       "      <td>0.920325</td>\n",
       "      <td>0.919222</td>\n",
       "      <td>0.899578</td>\n",
       "      <td>0.869778</td>\n",
       "      <td>0.897859</td>\n",
       "      <td>0.804122</td>\n",
       "      <td>...</td>\n",
       "      <td>1.478181</td>\n",
       "      <td>1.424749</td>\n",
       "      <td>1.485895</td>\n",
       "      <td>1.522987</td>\n",
       "      <td>1.604575</td>\n",
       "      <td>1.504297</td>\n",
       "      <td>1.443408</td>\n",
       "      <td>1.627278</td>\n",
       "      <td>1.838320</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>1997</td>\n",
       "      <td>0.054114</td>\n",
       "      <td>0.033520</td>\n",
       "      <td>0.073044</td>\n",
       "      <td>0.066204</td>\n",
       "      <td>0.091166</td>\n",
       "      <td>0.067316</td>\n",
       "      <td>0.059143</td>\n",
       "      <td>0.071504</td>\n",
       "      <td>0.048111</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.570693</td>\n",
       "      <td>-1.596148</td>\n",
       "      <td>-1.588646</td>\n",
       "      <td>-1.506543</td>\n",
       "      <td>-1.528152</td>\n",
       "      <td>-1.574358</td>\n",
       "      <td>-1.553255</td>\n",
       "      <td>-1.509639</td>\n",
       "      <td>-1.480995</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>1998</td>\n",
       "      <td>0.600298</td>\n",
       "      <td>0.618017</td>\n",
       "      <td>0.612976</td>\n",
       "      <td>0.707093</td>\n",
       "      <td>0.653833</td>\n",
       "      <td>0.689289</td>\n",
       "      <td>0.560395</td>\n",
       "      <td>0.688542</td>\n",
       "      <td>0.469316</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.536508</td>\n",
       "      <td>-0.561895</td>\n",
       "      <td>-0.448377</td>\n",
       "      <td>-0.409275</td>\n",
       "      <td>-0.546287</td>\n",
       "      <td>-0.461651</td>\n",
       "      <td>-0.484637</td>\n",
       "      <td>-0.506714</td>\n",
       "      <td>-0.257392</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>1999</td>\n",
       "      <td>0.479059</td>\n",
       "      <td>0.483076</td>\n",
       "      <td>0.685225</td>\n",
       "      <td>0.547489</td>\n",
       "      <td>0.755660</td>\n",
       "      <td>0.600390</td>\n",
       "      <td>0.634043</td>\n",
       "      <td>0.552527</td>\n",
       "      <td>0.525925</td>\n",
       "      <td>...</td>\n",
       "      <td>0.580215</td>\n",
       "      <td>0.506094</td>\n",
       "      <td>0.589203</td>\n",
       "      <td>0.689724</td>\n",
       "      <td>0.641492</td>\n",
       "      <td>0.559468</td>\n",
       "      <td>0.616796</td>\n",
       "      <td>0.501269</td>\n",
       "      <td>0.647127</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows × 62 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0  CNN_KFold_0  CNN_KFold_1  CNN_KFold_2  CNN_KFold_3  \\\n",
       "0              0     0.486822     0.522864     0.619165     0.621922   \n",
       "1              1     0.919098     0.916081     0.904019     0.927956   \n",
       "2              2     0.199294     0.228452     0.256258     0.443887   \n",
       "3              3     0.540013     0.486236     0.487673     0.521803   \n",
       "4              4     0.836471     0.806130     0.798109     0.827323   \n",
       "...          ...          ...          ...          ...          ...   \n",
       "1995        1995     0.142740     0.145378     0.300693     0.224134   \n",
       "1996        1996     0.849183     0.891769     0.900345     0.920325   \n",
       "1997        1997     0.054114     0.033520     0.073044     0.066204   \n",
       "1998        1998     0.600298     0.618017     0.612976     0.707093   \n",
       "1999        1999     0.479059     0.483076     0.685225     0.547489   \n",
       "\n",
       "      CNN_KFold_4  CNN_KFold_5  CNN_KFold_6  CNN_KFold_7  CNN_KFold_8  ...  \\\n",
       "0        0.671618     0.606191     0.576225     0.600986     0.482496  ...   \n",
       "1        0.931615     0.932323     0.883166     0.924349     0.827342  ...   \n",
       "2        0.354461     0.215887     0.270216     0.283867     0.208107  ...   \n",
       "3        0.563720     0.636915     0.476417     0.665422     0.398513  ...   \n",
       "4        0.856633     0.864365     0.767971     0.864445     0.682569  ...   \n",
       "...           ...          ...          ...          ...          ...  ...   \n",
       "1995     0.347749     0.163878     0.292498     0.160482     0.213052  ...   \n",
       "1996     0.919222     0.899578     0.869778     0.897859     0.804122  ...   \n",
       "1997     0.091166     0.067316     0.059143     0.071504     0.048111  ...   \n",
       "1998     0.653833     0.689289     0.560395     0.688542     0.469316  ...   \n",
       "1999     0.755660     0.600390     0.634043     0.552527     0.525925  ...   \n",
       "\n",
       "      SVM_KFold_1  SVM_KFold_2  SVM_KFold_3  SVM_KFold_4  SVM_KFold_5  \\\n",
       "0        0.401630     0.583595     0.576984     0.519101     0.511225   \n",
       "1        2.217742     1.874554     2.009260     1.979583     1.912362   \n",
       "2       -0.980761    -1.095997    -1.237363    -0.961593    -1.126832   \n",
       "3       -0.104977    -0.164274    -0.162744    -0.116986    -0.173945   \n",
       "4        0.958914     0.873867     0.853417     0.859337     0.902361   \n",
       "...           ...          ...          ...          ...          ...   \n",
       "1995    -1.096920    -1.131857    -1.115432    -0.862313    -1.039431   \n",
       "1996     1.478181     1.424749     1.485895     1.522987     1.604575   \n",
       "1997    -1.570693    -1.596148    -1.588646    -1.506543    -1.528152   \n",
       "1998    -0.536508    -0.561895    -0.448377    -0.409275    -0.546287   \n",
       "1999     0.580215     0.506094     0.589203     0.689724     0.641492   \n",
       "\n",
       "      SVM_KFold_6  SVM_KFold_7  SVM_KFold_8  SVM_KFold_9  True  \n",
       "0        0.492760     0.493398     0.478668     0.458823   1.0  \n",
       "1        2.076056     1.920187     1.975787     2.030250   1.0  \n",
       "2       -1.074842    -1.013983    -1.019705    -0.916501   1.0  \n",
       "3        0.010671    -0.194698    -0.060514    -0.113337   1.0  \n",
       "4        1.035411     1.018865     1.006281     0.921924   1.0  \n",
       "...           ...          ...          ...          ...   ...  \n",
       "1995    -1.043558    -1.002878    -0.905958    -1.083364   0.0  \n",
       "1996     1.504297     1.443408     1.627278     1.838320   0.0  \n",
       "1997    -1.574358    -1.553255    -1.509639    -1.480995   0.0  \n",
       "1998    -0.461651    -0.484637    -0.506714    -0.257392   0.0  \n",
       "1999     0.559468     0.616796     0.501269     0.647127   0.0  \n",
       "\n",
       "[2000 rows x 62 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(\"Result/loc_result_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a6c8ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:BERT]",
   "language": "python",
   "name": "conda-env-BERT-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
